{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " #!conda install -c conda-forge imbalanced-learn --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,  roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59129, 12)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.read_csv('datasets/combined.csv')\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X and y columns we need\n",
    "df =  combined[['Subreddit', 'preprocessed_words']]\n",
    "\n",
    "# add label for classification\n",
    "df['is_amd'] = df['Subreddit'].apply(lambda x: 1 if x == \"AMD\" else 0)\n",
    "df = df.drop(columns = 'Subreddit')\n",
    "df = df.rename(columns={'preprocessed_words':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_amd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keeps its price from the vram it has a is a ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s not you just a combo of the game running ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>starfield is cpu heavy which one do you have a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the game is just awfully optimized i just upgr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i tried using fsr but i did nt see any noticib...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_amd\n",
       "0  keeps its price from the vram it has a is a ta...       0\n",
       "1  it s not you just a combo of the game running ...       0\n",
       "2  starfield is cpu heavy which one do you have a...       0\n",
       "3  the game is just awfully optimized i just upgr...       0\n",
       "4  i tried using fsr but i did nt see any noticib...       0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Stopwords\n",
    "custom_stopwords = [ \"subreddit\", \"reddit\"]  # remove these words as it is not meaningful for our analysis\n",
    "stopwords_list = list(set(stopwords.words('english') + custom_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook on Modelling, I will only be considering the \"text\" column of the scraped dataset, and this has been pre-processed in notebook 2. This ensures that our model can be properly trained on the content of the subreddit posts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline \n",
    "We always begin with creating a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_amd\n",
       "1    0.634545\n",
       "0    0.365455\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model\n",
    "X = df['text']\n",
    "y = df['is_amd']\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Steps I took for this section:\n",
    "1. Train Test Split\n",
    "2. Instantiating Vectorizers and Models\n",
    "3. Creating a User Define Function* with Scikit-learn's Pipeline tool that will help calculate the relevant classification metrics from each model (Metrics include Accuracy, Specificity and F1_Score)\n",
    "4. Evaluate best model\n",
    "5. Tune Hyper-parameters of best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47303,)\n",
      "(11826,)\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify = y)\n",
    "X_train = X_train.values.astype('U')\n",
    "X_test = X_test.values.astype('U')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words=stopwords_list)\n",
    "cvec.fit(X_train)\n",
    "X_train = cvec.transform(X_train) #transform the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaaaa' ... 'zx' 'zz' 'zzx']\n",
      "(47303, 32124)\n"
     ]
    }
   ],
   "source": [
    "print(cvec.get_feature_names_out())\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test\n",
    "X_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify = y)\n",
    "X_train = X_train.values.astype('U')\n",
    "X_test = X_test.values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Baseline model, we can see moderate imbalance in our dataset (65%-35%). Hence, we can use the SMOTE technique to correct this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<47303x32124 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 842863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords_list)\n",
    "\n",
    "# Transform the text data into a TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<47303x32124 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 842863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "# Initialize the cvec vectorizer\n",
    "cvec_vectorizer = CountVectorizer(stop_words=stopwords_list)\n",
    "\n",
    "# Transform the text data into a cvec matrix\n",
    "cvec_matrix = cvec_vectorizer.fit_transform(X_train)\n",
    "cvec_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now we can create synthetic data for our training set\n",
    "\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "Xsm_train_t, ysm_train_t = sm.fit_resample(tfidf_matrix, y_train)\n",
    "# Xsm_train, ysm_train = sm.fit_resample(X_train_try, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now we can create synthetic data for our training set\n",
    "\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "Xsm_train_c, ysm_train_c = sm.fit_resample(cvec_matrix, y_train)\n",
    "# Xsm_train, ysm_train = sm.fit_resample(X_train_try, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ysm_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xsm_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11826x32124 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 203885 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11826x32124 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 203885 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cvec = cvec_vectorizer.transform(X_test)\n",
    "X_test_cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Random Forest classifier\n",
    "# clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Fit the classifier to your sparse training data\n",
    "# clf.fit(Xsm_train, ysm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_test_tfidf)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# print(\"rf\")\n",
    "# # Calculate and print accuracy for the test set\n",
    "# accuracy_test = accuracy_score(y_test, y_pred)\n",
    "# print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "# # Calculate and print specificity (true negative rate) for the test set\n",
    "# confusion_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "# tn, fp, fn, tp = confusion_matrix_test.ravel()\n",
    "# specificity = tn / (tn + fp)\n",
    "# print(\"Specificity:\", specificity)\n",
    "\n",
    "# # Calculate and print F1 score for the test set\n",
    "# f1_score_test = classification_report(y_test, y_pred, target_names=['class 0', 'class 1'], output_dict=True)['class 1']['f1-score']\n",
    "# print(\"F1 Score:\", f1_score_test)\n",
    "\n",
    "# # Calculate and print ROC AUC score for the test set\n",
    "# roc_auc = roc_auc_score(y_test, clf.predict_proba(X_test_tfidf)[:, 1])\n",
    "# print(\"ROC AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Instantiation of Vectorizers and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be exploring different Classification algorithms and using both Count Vectorizer or Term Frequency-Inverse Document Frequency (TFIDF) transformers:\n",
    "- Count Vectorizer: Takes every word as a token, and uses it as a feature.\n",
    "- TFIFD: accounts for frequency of a word in a given document and the frequency between documents. Word importance increases proportionally to the number of times it appears in a document, but is offset by frequency of word in entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Vectorizers\n",
    "vectorizers = {'cvec': CountVectorizer(stop_words=stopwords_list),\n",
    "               'tvec': TfidfVectorizer(stop_words=stopwords_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instiantiate models\n",
    "models = {'nb': MultinomialNB(),\n",
    "          'log_reg': LogisticRegression(max_iter=500, random_state=123),\n",
    "          'rf': RandomForestClassifier(random_state=123),\n",
    "          'knn': KNeighborsClassifier()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. User Define Function - inputs required are vectorizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smote_results = []\n",
    "\n",
    "def clf_model(vec, mod, cv_num, X_train, y_train):   # option to include Grid Search\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "            (mod, models[mod])\n",
    "            ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    print(vec)\n",
    "    if vec == 'tvec':\n",
    "        # Get predictions\n",
    "        preds = pipe.predict(X_test_tfidf)\n",
    "        \n",
    "    else:\n",
    "        # Get predictions\n",
    "        preds = pipe.predict(X_test_cvec)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "    acc = (tp + tn)/ (tp+tn+fp+fn)\n",
    "    spec = tn / (tn + fp)\n",
    "    \n",
    "    # Retrieve metrics\n",
    "    results['Model'] = mod\n",
    "    results['Vectorizer'] = vec\n",
    "    results['Train Score'] = pipe.score(X_train, y_train)\n",
    "\n",
    "    if vec=='tvec':\n",
    "        results['Test Score'] = pipe.score(X_test_tfidf, y_test)\n",
    "\n",
    "    else:\n",
    "        results['Test Score'] = pipe.score(X_test_cvec, y_test)\n",
    "        \n",
    "    results['Accuracy'] = acc\n",
    "    results['Specificity'] = spec\n",
    "    results['f_score'] = f1_score(y_test, preds)\n",
    "#     nb.predict_proba(X_test_tfidf)[:, 1]\n",
    "    results['ROC_AUC'] = roc_auc_score(y_test, preds)\n",
    "\n",
    "    df_smote_results.append(results)\n",
    "    \n",
    "    print(f\"--- METRICS for {mod},{vec} ---\")\n",
    "    display(results)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvec\n",
      "--- METRICS for nb,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.7912280117270789,\n",
       " 'Test Score': 0.7168949771689498,\n",
       " 'Accuracy': 0.7168949771689498,\n",
       " 'Specificity': 0.7117075428042573,\n",
       " 'f_score': 0.7634256642170717,\n",
       " 'ROC_AUC': 0.7157951360076723}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvec\n",
      "--- METRICS for nb,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.7987406716417911,\n",
       " 'Test Score': 0.7241670894638931,\n",
       " 'Accuracy': 0.7241670894638931,\n",
       " 'Specificity': 0.6672836649699213,\n",
       " 'f_score': 0.7769115032143346,\n",
       " 'ROC_AUC': 0.7121066512482868}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "cvec_nb = clf_model('cvec', 'nb', 5,Xsm_train_c,ysm_train_c)\n",
    "tvec_nb = clf_model('tvec', 'nb', 5, Xsm_train_t, ysm_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xsm_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\project_3\\3-web-scraping_v1 anusha.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Xsm_train\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xsm_train' is not defined"
     ]
    }
   ],
   "source": [
    "Xsm_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvec\n",
      "--- METRICS for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.8447827825159915,\n",
       " 'Test Score': 0.6932183324877389,\n",
       " 'Accuracy': 0.6932183324877389,\n",
       " 'Specificity': 0.7265155020823693,\n",
       " 'f_score': 0.7360302677532015,\n",
       " 'ROC_AUC': 0.7002780069047241}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvec\n",
      "--- METRICS for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8107509328358209,\n",
       " 'Test Score': 0.7098765432098766,\n",
       " 'Accuracy': 0.7098765432098766,\n",
       " 'Specificity': 0.7024525682554373,\n",
       " 'f_score': 0.7575093646194078,\n",
       " 'ROC_AUC': 0.7083025101405117}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "cvec_lr = clf_model('cvec', 'log_reg', 5,Xsm_train_c,ysm_train_c)\n",
    "tvec_lr = clf_model('tvec', 'log_reg', 5, Xsm_train_t, ysm_train_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvec\n",
      "--- METRICS for rf,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'rf',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9873234275053305,\n",
       " 'Test Score': 0.6727549467275494,\n",
       " 'Accuracy': 0.6727549467275494,\n",
       " 'Specificity': 0.5920869967607589,\n",
       " 'f_score': 0.736088379705401,\n",
       " 'ROC_AUC': 0.6556517073356034}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvec\n",
      "--- METRICS for rf,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'rf',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.9905050639658849,\n",
       " 'Test Score': 0.7106375782174869,\n",
       " 'Accuracy': 0.7106375782174869,\n",
       " 'Specificity': 0.5918556223970384,\n",
       " 'f_score': 0.7735874024083632,\n",
       " 'ROC_AUC': 0.6854533975524638}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression using SMOTE data\n",
    "cvec_lr = clf_model('cvec', 'rf', 5, Xsm_train_c, ysm_train_c)\n",
    "tvec_lr = clf_model('tvec', 'rf', 5,Xsm_train_t, ysm_train_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\project_3\\3-web-scraping_v1 anusha.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# KNN using SMOTE data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cvec_knn \u001b[39m=\u001b[39m clf_model(\u001b[39m'\u001b[39;49m\u001b[39mcvec\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mknn\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m5\u001b[39;49m, Xsm_train_c, ysm_train_c)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tvec_knn \u001b[39m=\u001b[39m clf_model(\u001b[39m'\u001b[39m\u001b[39mtvec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mknn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m5\u001b[39m, Xsm_train_t, ysm_train_t)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(df_smote_results)\n",
      "\u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\project_3\\3-web-scraping_v1 anusha.ipynb Cell 41\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mTest Score\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mscore(X_test_tfidf, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mTest Score\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49mscore(X_test_cvec, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m results[\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m acc\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1%20anusha.ipynb#X55sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m results[\u001b[39m'\u001b[39m\u001b[39mSpecificity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m spec\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\pipeline.py:722\u001b[0m, in \u001b[0;36mPipeline.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     score_params[\u001b[39m\"\u001b[39m\u001b[39msample_weight\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sample_weight\n\u001b[1;32m--> 722\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mscore(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mscore_params)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\base.py:668\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 668\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:234\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39m    Class labels for each data sample.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    232\u001b[0m     \u001b[39m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[39m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    235\u001b[0m     neigh_dist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:861\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    858\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m         kwds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_params_\n\u001b[1;32m--> 861\u001b[0m     chunked_results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m    862\u001b[0m         pairwise_distances_chunked(\n\u001b[0;32m    863\u001b[0m             X,\n\u001b[0;32m    864\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[0;32m    865\u001b[0m             reduce_func\u001b[39m=\u001b[39;49mreduce_func,\n\u001b[0;32m    866\u001b[0m             metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[0;32m    867\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    868\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mball_tree\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkd_tree\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    873\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1876\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1874\u001b[0m \u001b[39mif\u001b[39;00m reduce_func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1875\u001b[0m     chunk_size \u001b[39m=\u001b[39m D_chunk\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1876\u001b[0m     D_chunk \u001b[39m=\u001b[39m reduce_func(D_chunk, sl\u001b[39m.\u001b[39;49mstart)\n\u001b[0;32m   1877\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[0;32m   1878\u001b[0m \u001b[39myield\u001b[39;00m D_chunk\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:719\u001b[0m, in \u001b[0;36mKNeighborsMixin._kneighbors_reduce_func\u001b[1;34m(self, dist, start, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reduce a chunk of distances to the nearest neighbors.\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \n\u001b[0;32m    694\u001b[0m \u001b[39mCallback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39m    The neighbors indices.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    718\u001b[0m sample_range \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(dist\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])[:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m--> 719\u001b[0m neigh_ind \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margpartition(dist, n_neighbors \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    720\u001b[0m neigh_ind \u001b[39m=\u001b[39m neigh_ind[:, :n_neighbors]\n\u001b[0;32m    721\u001b[0m \u001b[39m# argpartition doesn't guarantee sorted order, so we sort again\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:858\u001b[0m, in \u001b[0;36margpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[0;32m    780\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margpartition\u001b[39m(a, kth, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mintroselect\u001b[39m\u001b[39m'\u001b[39m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    781\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[39m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[39m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    856\u001b[0m \n\u001b[0;32m    857\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 858\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margpartition\u001b[39;49m\u001b[39m'\u001b[39;49m, kth, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# KNN using SMOTE data\n",
    "cvec_knn = clf_model('cvec', 'knn', 5, Xsm_train_c, ysm_train_c)\n",
    "tvec_knn = clf_model('tvec', 'knn', 5, Xsm_train_t, ysm_train_t)\n",
    "pd.DataFrame(df_smote_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Hyperparameter Tuning of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a robust machine learning model, we need to select the correct combination of hyperparameters.\n",
    "Recall that in the user defined function created earlier, if \"Gridsearch\" = True, the function will perform a gridsearch to find the optimal hyperparameters that will give the best score. GridsearchCV searches all combinations of paramters in for a model that will give the best peformance score. It is not practically feasible to run a GridSearchCV on all models due to the complexity and hence time taken. Hence, I will select only the best 3 models to perform hyperparameter tuning.\n",
    "\n",
    "From the above results table, seems like the Multinomial Naive Bayes Model, Logistic Regression and Random Forest perform the best.\n",
    "\n",
    "- Multinomial Naive Bayes: Based on Bayes's theorem - the assumption that each feature (in our case, each word) is independent of each other.\n",
    "- Logistic Regression:\n",
    "- Random Forest: Consists of n number of decision trees that act as an ensemble. Each decision tree makes a class prediction and the class with the most votes becomes the model's prediction.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = []\n",
    "\n",
    "# Vectorizer Parameters\n",
    "\n",
    "cvec_params = {\n",
    "    'cvec__max_features': [None],\n",
    "    'cvec__min_df':[3, 4, 5],\n",
    "    'cvec__max_df': [0.2, 0.3, 0.4],\n",
    "    'cvec__stop_words': [stopwords_list],\n",
    "    'cvec__ngram_range':[(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "\n",
    "tvec_params = {\n",
    "    'tvec__max_features': [None],\n",
    "    'tvec__min_df':[3, 4, 5],\n",
    "    'tvec__max_df': [0.2, 0.3, 0.4],\n",
    "    'tvec__stop_words': [stopwords_list],\n",
    "    'tvec__ngram_range':[(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "rf_pipe_cvec_params = {\n",
    "    'cvec__max_features': [100],\n",
    "    'cvec__max_df': [0.2, 0.3],\n",
    "    'cvec__min_df': [1, 2, 3],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [50,75,100]}\n",
    "\n",
    "\n",
    "rf_pipe_tvec_params = {\n",
    "    'tvec__max_features': [100],\n",
    "    'tvec__max_df': [0.2, 0.3],\n",
    "    'tvec__min_df': [1, 2, 3],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [50,75,100]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "--- Best Parameters for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.3,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': ['of',\n",
       "  'o',\n",
       "  'if',\n",
       "  'most',\n",
       "  'as',\n",
       "  'into',\n",
       "  'yourselves',\n",
       "  'any',\n",
       "  \"weren't\",\n",
       "  'ours',\n",
       "  'itself',\n",
       "  'that',\n",
       "  'our',\n",
       "  'doesn',\n",
       "  \"aren't\",\n",
       "  'd',\n",
       "  'having',\n",
       "  'out',\n",
       "  \"shan't\",\n",
       "  'both',\n",
       "  \"you're\",\n",
       "  'other',\n",
       "  'here',\n",
       "  \"shouldn't\",\n",
       "  'all',\n",
       "  'she',\n",
       "  'aren',\n",
       "  'off',\n",
       "  'herself',\n",
       "  'yourself',\n",
       "  'should',\n",
       "  'll',\n",
       "  'were',\n",
       "  \"needn't\",\n",
       "  'how',\n",
       "  'a',\n",
       "  'about',\n",
       "  'hasn',\n",
       "  \"haven't\",\n",
       "  'whom',\n",
       "  \"she's\",\n",
       "  'nor',\n",
       "  'wasn',\n",
       "  'ma',\n",
       "  'now',\n",
       "  'by',\n",
       "  'them',\n",
       "  \"don't\",\n",
       "  'y',\n",
       "  'what',\n",
       "  \"doesn't\",\n",
       "  'can',\n",
       "  'they',\n",
       "  'being',\n",
       "  'not',\n",
       "  'been',\n",
       "  'no',\n",
       "  'which',\n",
       "  'than',\n",
       "  \"won't\",\n",
       "  'mightn',\n",
       "  'its',\n",
       "  'during',\n",
       "  'between',\n",
       "  'why',\n",
       "  'because',\n",
       "  'shouldn',\n",
       "  'before',\n",
       "  'or',\n",
       "  'the',\n",
       "  \"mightn't\",\n",
       "  'against',\n",
       "  'don',\n",
       "  \"couldn't\",\n",
       "  'hadn',\n",
       "  'very',\n",
       "  'does',\n",
       "  'your',\n",
       "  'have',\n",
       "  'but',\n",
       "  'down',\n",
       "  'couldn',\n",
       "  'subreddit',\n",
       "  'on',\n",
       "  'at',\n",
       "  'his',\n",
       "  \"you'll\",\n",
       "  'to',\n",
       "  'then',\n",
       "  'where',\n",
       "  \"you'd\",\n",
       "  'this',\n",
       "  'her',\n",
       "  'own',\n",
       "  \"you've\",\n",
       "  'once',\n",
       "  \"that'll\",\n",
       "  'so',\n",
       "  'under',\n",
       "  'won',\n",
       "  'it',\n",
       "  'only',\n",
       "  's',\n",
       "  'through',\n",
       "  \"didn't\",\n",
       "  \"hasn't\",\n",
       "  'until',\n",
       "  \"wasn't\",\n",
       "  'mustn',\n",
       "  \"wouldn't\",\n",
       "  'over',\n",
       "  'up',\n",
       "  'do',\n",
       "  'few',\n",
       "  'hers',\n",
       "  'is',\n",
       "  'will',\n",
       "  'shan',\n",
       "  'me',\n",
       "  'themselves',\n",
       "  'reddit',\n",
       "  'each',\n",
       "  'ourselves',\n",
       "  'myself',\n",
       "  'my',\n",
       "  'after',\n",
       "  'from',\n",
       "  've',\n",
       "  'their',\n",
       "  'while',\n",
       "  'needn',\n",
       "  'weren',\n",
       "  'himself',\n",
       "  \"hadn't\",\n",
       "  'theirs',\n",
       "  \"mustn't\",\n",
       "  'in',\n",
       "  'are',\n",
       "  'ain',\n",
       "  'am',\n",
       "  'more',\n",
       "  'didn',\n",
       "  'was',\n",
       "  'with',\n",
       "  'you',\n",
       "  'there',\n",
       "  'isn',\n",
       "  'same',\n",
       "  'has',\n",
       "  'when',\n",
       "  't',\n",
       "  \"it's\",\n",
       "  'yours',\n",
       "  'did',\n",
       "  'he',\n",
       "  'doing',\n",
       "  'some',\n",
       "  'haven',\n",
       "  'and',\n",
       "  'too',\n",
       "  'these',\n",
       "  'those',\n",
       "  'for',\n",
       "  'm',\n",
       "  'him',\n",
       "  'above',\n",
       "  'such',\n",
       "  'wouldn',\n",
       "  'further',\n",
       "  'had',\n",
       "  'below',\n",
       "  'just',\n",
       "  'i',\n",
       "  \"isn't\",\n",
       "  're',\n",
       "  \"should've\",\n",
       "  'again',\n",
       "  'an',\n",
       "  'who',\n",
       "  'be',\n",
       "  'we']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9349724118977655,\n",
       " 'Test Score': 0.7404870624048706,\n",
       " 'Accuracy': 0.7404870624048706,\n",
       " 'Specificity': 0.5552984729291994,\n",
       " 'f_score': 0.8055502756130014,\n",
       " 'ROC_AUC': 0.7012233302812308}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "--- Best Parameters for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.2,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 3,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': ['of',\n",
       "  'o',\n",
       "  'if',\n",
       "  'most',\n",
       "  'as',\n",
       "  'into',\n",
       "  'yourselves',\n",
       "  'any',\n",
       "  \"weren't\",\n",
       "  'ours',\n",
       "  'itself',\n",
       "  'that',\n",
       "  'our',\n",
       "  'doesn',\n",
       "  \"aren't\",\n",
       "  'd',\n",
       "  'having',\n",
       "  'out',\n",
       "  \"shan't\",\n",
       "  'both',\n",
       "  \"you're\",\n",
       "  'other',\n",
       "  'here',\n",
       "  \"shouldn't\",\n",
       "  'all',\n",
       "  'she',\n",
       "  'aren',\n",
       "  'off',\n",
       "  'herself',\n",
       "  'yourself',\n",
       "  'should',\n",
       "  'll',\n",
       "  'were',\n",
       "  \"needn't\",\n",
       "  'how',\n",
       "  'a',\n",
       "  'about',\n",
       "  'hasn',\n",
       "  \"haven't\",\n",
       "  'whom',\n",
       "  \"she's\",\n",
       "  'nor',\n",
       "  'wasn',\n",
       "  'ma',\n",
       "  'now',\n",
       "  'by',\n",
       "  'them',\n",
       "  \"don't\",\n",
       "  'y',\n",
       "  'what',\n",
       "  \"doesn't\",\n",
       "  'can',\n",
       "  'they',\n",
       "  'being',\n",
       "  'not',\n",
       "  'been',\n",
       "  'no',\n",
       "  'which',\n",
       "  'than',\n",
       "  \"won't\",\n",
       "  'mightn',\n",
       "  'its',\n",
       "  'during',\n",
       "  'between',\n",
       "  'why',\n",
       "  'because',\n",
       "  'shouldn',\n",
       "  'before',\n",
       "  'or',\n",
       "  'the',\n",
       "  \"mightn't\",\n",
       "  'against',\n",
       "  'don',\n",
       "  \"couldn't\",\n",
       "  'hadn',\n",
       "  'very',\n",
       "  'does',\n",
       "  'your',\n",
       "  'have',\n",
       "  'but',\n",
       "  'down',\n",
       "  'couldn',\n",
       "  'subreddit',\n",
       "  'on',\n",
       "  'at',\n",
       "  'his',\n",
       "  \"you'll\",\n",
       "  'to',\n",
       "  'then',\n",
       "  'where',\n",
       "  \"you'd\",\n",
       "  'this',\n",
       "  'her',\n",
       "  'own',\n",
       "  \"you've\",\n",
       "  'once',\n",
       "  \"that'll\",\n",
       "  'so',\n",
       "  'under',\n",
       "  'won',\n",
       "  'it',\n",
       "  'only',\n",
       "  's',\n",
       "  'through',\n",
       "  \"didn't\",\n",
       "  \"hasn't\",\n",
       "  'until',\n",
       "  \"wasn't\",\n",
       "  'mustn',\n",
       "  \"wouldn't\",\n",
       "  'over',\n",
       "  'up',\n",
       "  'do',\n",
       "  'few',\n",
       "  'hers',\n",
       "  'is',\n",
       "  'will',\n",
       "  'shan',\n",
       "  'me',\n",
       "  'themselves',\n",
       "  'reddit',\n",
       "  'each',\n",
       "  'ourselves',\n",
       "  'myself',\n",
       "  'my',\n",
       "  'after',\n",
       "  'from',\n",
       "  've',\n",
       "  'their',\n",
       "  'while',\n",
       "  'needn',\n",
       "  'weren',\n",
       "  'himself',\n",
       "  \"hadn't\",\n",
       "  'theirs',\n",
       "  \"mustn't\",\n",
       "  'in',\n",
       "  'are',\n",
       "  'ain',\n",
       "  'am',\n",
       "  'more',\n",
       "  'didn',\n",
       "  'was',\n",
       "  'with',\n",
       "  'you',\n",
       "  'there',\n",
       "  'isn',\n",
       "  'same',\n",
       "  'has',\n",
       "  'when',\n",
       "  't',\n",
       "  \"it's\",\n",
       "  'yours',\n",
       "  'did',\n",
       "  'he',\n",
       "  'doing',\n",
       "  'some',\n",
       "  'haven',\n",
       "  'and',\n",
       "  'too',\n",
       "  'these',\n",
       "  'those',\n",
       "  'for',\n",
       "  'm',\n",
       "  'him',\n",
       "  'above',\n",
       "  'such',\n",
       "  'wouldn',\n",
       "  'further',\n",
       "  'had',\n",
       "  'below',\n",
       "  'just',\n",
       "  'i',\n",
       "  \"isn't\",\n",
       "  're',\n",
       "  \"should've\",\n",
       "  'again',\n",
       "  'an',\n",
       "  'who',\n",
       "  'be',\n",
       "  'we']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8311523582013826,\n",
       " 'Test Score': 0.7434466429900219,\n",
       " 'Accuracy': 0.7434466429900219,\n",
       " 'Specificity': 0.49074502545118,\n",
       " 'f_score': 0.8147288715192965,\n",
       " 'ROC_AUC': 0.6898687813823065}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tune for Logistic Regression\n",
    "cvec_lr_gs = clf_model('cvec', 'log_reg', 3, vec_params=cvec_params, grid_search=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\project_3\\3-web-scraping_v1.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tvec_lr_gs \u001b[39m=\u001b[39m clf_model(\u001b[39m'\u001b[39;49m\u001b[39mtvec\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlog_reg\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m3\u001b[39;49m, vec_params\u001b[39m=\u001b[39;49mtvec_params, grid_search\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\project_3\\3-web-scraping_v1.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X62sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X62sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         gs \u001b[39m=\u001b[39m GridSearchCV(pipe, param_grid \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvec_params}, cv\u001b[39m=\u001b[39mcv_num, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X62sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         gs\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X62sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         pipe \u001b[39m=\u001b[39m gs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X62sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>f_score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.934972</td>\n",
       "      <td>0.740487</td>\n",
       "      <td>0.740487</td>\n",
       "      <td>0.555298</td>\n",
       "      <td>0.805550</td>\n",
       "      <td>0.701223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.831152</td>\n",
       "      <td>0.743447</td>\n",
       "      <td>0.743447</td>\n",
       "      <td>0.490745</td>\n",
       "      <td>0.814729</td>\n",
       "      <td>0.689869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model Vectorizer  Train Score  Test Score  Accuracy  Specificity  \\\n",
       "0  log_reg       cvec     0.934972    0.740487  0.740487     0.555298   \n",
       "1  log_reg       tvec     0.831152    0.743447  0.743447     0.490745   \n",
       "\n",
       "    f_score   ROC_AUC  \n",
       "0  0.805550  0.701223  \n",
       "1  0.814729  0.689869  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tuning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\project_3\\3-web-scraping_v1.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Tune for Random Forest using RandomizedSearchCV to improve model runtime\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cvec_rf_gs \u001b[39m=\u001b[39m clf_model(\u001b[39m'\u001b[39;49m\u001b[39mcvec\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrf\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m3\u001b[39;49m, vec_params\u001b[39m=\u001b[39;49mrf_pipe_cvec_params, grid_search\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\project_3\\3-web-scraping_v1.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m mod \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrf\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     rs \u001b[39m=\u001b[39m RandomizedSearchCV(pipe, param_distributions \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvec_params}, cv\u001b[39m=\u001b[39mcv_num, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m) \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     rs\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     pipe \u001b[39m=\u001b[39m rs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#X44sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:933\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    931\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    932\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 933\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\.conda\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tune for Random Forest using RandomizedSearchCV to improve model runtime\n",
    "cvec_rf_gs = clf_model('cvec', 'rf', 3, vec_params=rf_pipe_cvec_params, grid_search=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tiffa\\Documents\\DSIF-SG-11\\project_3\\3-web-scraping_v1.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tiffa/Documents/DSIF-SG-11/project_3/3-web-scraping_v1.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "tvec_rf_gs = clf_model('tvec', 'rf', 3, vec_params=rf_pipe_tvec_params, grid_search=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tuning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>CVEC</td>\n",
       "      <td>0.680676</td>\n",
       "      <td>0.681549</td>\n",
       "      <td>0.670726</td>\n",
       "      <td>0.217955</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TVEC</td>\n",
       "      <td>0.672114</td>\n",
       "      <td>0.670726</td>\n",
       "      <td>0.670726</td>\n",
       "      <td>0.217955</td>\n",
       "      <td>0.787568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Vectorizer  Train Score  Test Score  Accuracy  \\\n",
       "0  Logistic Regression       CVEC     0.680676    0.681549  0.670726   \n",
       "1  Logistic Regression       TVEC     0.672114    0.670726  0.670726   \n",
       "\n",
       "   Specificity  F1 Score  \n",
       "0     0.217955       NaN  \n",
       "1     0.217955  0.787568  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tvec_results = {\n",
    "    'Model': \"Logistic Regression\",\n",
    "    'Vectorizer': \"TVEC\",\n",
    "    'Train Score': tvec_lr_gs.score(X_train, y_train),\n",
    "    'Test Score': tvec_lr_gs.score(X_test, y_test),\n",
    "    'Accuracy': acc,\n",
    "    'Specificity': spec,\n",
    "    'F1 Score': f1_score\n",
    "}\n",
    "\n",
    "results_df.append(lr_tvec_results)\n",
    "pd.DataFrame(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Random Forest using *TFIFD Vectorizer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
