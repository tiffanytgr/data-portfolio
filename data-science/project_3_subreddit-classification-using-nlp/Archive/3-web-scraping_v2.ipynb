{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III: Modelling & Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write an intro here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #!conda install -c conda-forge imbalanced-learn --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,  roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,HalvingGridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60778, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.read_csv('datasets/combined.csv')\n",
    "\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>preprocessed_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16d4gg0</td>\n",
       "      <td>2023-09-08 08:04:04</td>\n",
       "      <td>New to pc building</td>\n",
       "      <td>What’s better rtx 3090 for 700$-I’ve seen thes...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43VerLoner</td>\n",
       "      <td>3090 keeps its price from the VRAM it has. A 4...</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>['3090 keeps its price from the VRAM it has.',...</td>\n",
       "      <td>['3090', 'keeps', 'its', 'price', 'from', 'the...</td>\n",
       "      <td>keeps its price from the vram it has a is a ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16d46z5</td>\n",
       "      <td>2023-09-08 07:47:35</td>\n",
       "      <td>Experiences</td>\n",
       "      <td>I personally play on a typical 1080ti system(...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>SEALEJ2001</td>\n",
       "      <td>It's not you, just a combo of the game running...</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>[\"It's not you, just a combo of the game runni...</td>\n",
       "      <td>['It', \"'s\", 'not', 'you', ',', 'just', 'a', '...</td>\n",
       "      <td>it s not you just a combo of the game running ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16d46z5</td>\n",
       "      <td>2023-09-08 07:47:35</td>\n",
       "      <td>Experiences</td>\n",
       "      <td>I personally play on a typical 1080ti system(...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>SEALEJ2001</td>\n",
       "      <td>Starfield is CPU heavy, which one do you have?...</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>['Starfield is CPU heavy, which one do you hav...</td>\n",
       "      <td>['Starfield', 'is', 'CPU', 'heavy', ',', 'whic...</td>\n",
       "      <td>starfield is cpu heavy which one do you have a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>16d46z5</td>\n",
       "      <td>2023-09-08 07:47:35</td>\n",
       "      <td>Experiences</td>\n",
       "      <td>I personally play on a typical 1080ti system(...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>SEALEJ2001</td>\n",
       "      <td>The game is just awfully optimized. I just upg...</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>['The game is just awfully optimized.', 'I jus...</td>\n",
       "      <td>['The', 'game', 'is', 'just', 'awfully', 'opti...</td>\n",
       "      <td>the game is just awfully optimized i just upgr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16d46z5</td>\n",
       "      <td>2023-09-08 07:47:35</td>\n",
       "      <td>Experiences</td>\n",
       "      <td>I personally play on a typical 1080ti system(...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>SEALEJ2001</td>\n",
       "      <td>I tried using fsr but I didn't see any noticib...</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>[\"I tried using fsr but I didn't see any notic...</td>\n",
       "      <td>['I', 'tried', 'using', 'fsr', 'but', 'I', 'di...</td>\n",
       "      <td>i tried using fsr but i did nt see any noticib...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                 date               title  \\\n",
       "0           0  16d4gg0  2023-09-08 08:04:04  New to pc building   \n",
       "1           1  16d46z5  2023-09-08 07:47:35         Experiences   \n",
       "2           2  16d46z5  2023-09-08 07:47:35         Experiences   \n",
       "3           3  16d46z5  2023-09-08 07:47:35         Experiences   \n",
       "4           4  16d46z5  2023-09-08 07:47:35         Experiences   \n",
       "\n",
       "                                            selftext  n_comments      author  \\\n",
       "0  What’s better rtx 3090 for 700$-I’ve seen thes...         1.0  43VerLoner   \n",
       "1   I personally play on a typical 1080ti system(...         7.0  SEALEJ2001   \n",
       "2   I personally play on a typical 1080ti system(...         7.0  SEALEJ2001   \n",
       "3   I personally play on a typical 1080ti system(...         7.0  SEALEJ2001   \n",
       "4   I personally play on a typical 1080ti system(...         7.0  SEALEJ2001   \n",
       "\n",
       "                                             comment Subreddit  \\\n",
       "0  3090 keeps its price from the VRAM it has. A 4...    Nvidia   \n",
       "1  It's not you, just a combo of the game running...    Nvidia   \n",
       "2  Starfield is CPU heavy, which one do you have?...    Nvidia   \n",
       "3  The game is just awfully optimized. I just upg...    Nvidia   \n",
       "4  I tried using fsr but I didn't see any noticib...    Nvidia   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  ['3090 keeps its price from the VRAM it has.',...   \n",
       "1  [\"It's not you, just a combo of the game runni...   \n",
       "2  ['Starfield is CPU heavy, which one do you hav...   \n",
       "3  ['The game is just awfully optimized.', 'I jus...   \n",
       "4  [\"I tried using fsr but I didn't see any notic...   \n",
       "\n",
       "                                               words  \\\n",
       "0  ['3090', 'keeps', 'its', 'price', 'from', 'the...   \n",
       "1  ['It', \"'s\", 'not', 'you', ',', 'just', 'a', '...   \n",
       "2  ['Starfield', 'is', 'CPU', 'heavy', ',', 'whic...   \n",
       "3  ['The', 'game', 'is', 'just', 'awfully', 'opti...   \n",
       "4  ['I', 'tried', 'using', 'fsr', 'but', 'I', 'di...   \n",
       "\n",
       "                                  preprocessed_words  \n",
       "0  keeps its price from the vram it has a is a ta...  \n",
       "1  it s not you just a combo of the game running ...  \n",
       "2  starfield is cpu heavy which one do you have a...  \n",
       "3  the game is just awfully optimized i just upgr...  \n",
       "4  i tried using fsr but i did nt see any noticib...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X and y columns we need\n",
    "df =  combined[['Subreddit', 'preprocessed_words']]\n",
    "\n",
    "# add label for classification\n",
    "df['is_amd'] = df['Subreddit'].apply(lambda x: 1 if x == \"AMD\" else 0)\n",
    "df = df.drop(columns = 'Subreddit')\n",
    "df = df.rename(columns={'preprocessed_words':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_amd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keeps its price from the vram it has a is a ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s not you just a combo of the game running ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>starfield is cpu heavy which one do you have a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the game is just awfully optimized i just upgr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i tried using fsr but i did nt see any noticib...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_amd\n",
       "0  keeps its price from the vram it has a is a ta...       0\n",
       "1  it s not you just a combo of the game running ...       0\n",
       "2  starfield is cpu heavy which one do you have a...       0\n",
       "3  the game is just awfully optimized i just upgr...       0\n",
       "4  i tried using fsr but i did nt see any noticib...       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Stopwords\n",
    "custom_stopwords = [ \"subreddit\", \"reddit\"]  # remove these words as it is not meaningful for our analysis\n",
    "stopwords_list = list(set(stopwords.words('english') + custom_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also import the top common words of both subreddits from Notebook 2\n",
    "common_words = pd.read_csv('datasets/common_words.csv',index_col=[0])\n",
    "common_words_30 = common_words.head(20)\n",
    "\n",
    "# Create an additional stopwords list that includes the top 10 common words\n",
    "stopwords_list_with_common =  list(set(stopwords.words('english') + custom_stopwords + common_words_30['0'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after',\n",
       " 'why',\n",
       " \"weren't\",\n",
       " \"shan't\",\n",
       " 'good',\n",
       " 'amd',\n",
       " 'nvidia',\n",
       " 'who',\n",
       " 'being',\n",
       " 'because',\n",
       " 'won',\n",
       " 'was',\n",
       " 'at',\n",
       " 'both',\n",
       " 'such',\n",
       " 'own',\n",
       " 'fps',\n",
       " 'it',\n",
       " 'hers',\n",
       " 'under',\n",
       " 'when',\n",
       " 'having',\n",
       " 'shouldn',\n",
       " 'subreddit',\n",
       " 'xt',\n",
       " 'hasn',\n",
       " 'wasn',\n",
       " 'did',\n",
       " 'should',\n",
       " 'fsr',\n",
       " 'for',\n",
       " 'those',\n",
       " 'were',\n",
       " 'a',\n",
       " 'no',\n",
       " \"you're\",\n",
       " 'what',\n",
       " 'game',\n",
       " 'weren',\n",
       " 'its',\n",
       " 'didn',\n",
       " 'would',\n",
       " 'only',\n",
       " 'out',\n",
       " \"don't\",\n",
       " 'on',\n",
       " 'their',\n",
       " 'this',\n",
       " 'or',\n",
       " 'very',\n",
       " 'they',\n",
       " 'her',\n",
       " \"hasn't\",\n",
       " 'o',\n",
       " 'can',\n",
       " \"mustn't\",\n",
       " 'hadn',\n",
       " 'the',\n",
       " 'we',\n",
       " 'from',\n",
       " 'doesn',\n",
       " 'your',\n",
       " \"haven't\",\n",
       " 'over',\n",
       " 'gpu',\n",
       " 'against',\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"it's\",\n",
       " 'up',\n",
       " \"doesn't\",\n",
       " 'to',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'once',\n",
       " 'which',\n",
       " 'below',\n",
       " 'am',\n",
       " 'how',\n",
       " 'other',\n",
       " 'shan',\n",
       " 'one',\n",
       " \"you'll\",\n",
       " 't',\n",
       " 'each',\n",
       " 'so',\n",
       " 'y',\n",
       " \"hadn't\",\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mustn',\n",
       " 'and',\n",
       " 'ain',\n",
       " 'be',\n",
       " 'd',\n",
       " 'than',\n",
       " 'as',\n",
       " 'needn',\n",
       " \"that'll\",\n",
       " \"should've\",\n",
       " \"you've\",\n",
       " \"she's\",\n",
       " 'ourselves',\n",
       " 're',\n",
       " 'wouldn',\n",
       " 'between',\n",
       " 'do',\n",
       " 'of',\n",
       " 'll',\n",
       " 'him',\n",
       " 'here',\n",
       " 'myself',\n",
       " \"needn't\",\n",
       " 'get',\n",
       " 'has',\n",
       " 'few',\n",
       " 'had',\n",
       " 'mightn',\n",
       " \"didn't\",\n",
       " 'ours',\n",
       " 'm',\n",
       " 'i',\n",
       " 'his',\n",
       " 'down',\n",
       " 'me',\n",
       " 'theirs',\n",
       " 'are',\n",
       " 'if',\n",
       " 'yourself',\n",
       " 'like',\n",
       " 'further',\n",
       " 'also',\n",
       " 'yourselves',\n",
       " 'then',\n",
       " 'even',\n",
       " 'same',\n",
       " 'an',\n",
       " 'but',\n",
       " 'my',\n",
       " 'he',\n",
       " 'whom',\n",
       " 'that',\n",
       " 'again',\n",
       " 'above',\n",
       " 'most',\n",
       " 'while',\n",
       " 'reddit',\n",
       " 'off',\n",
       " 'herself',\n",
       " 'in',\n",
       " 'where',\n",
       " 'more',\n",
       " 'themselves',\n",
       " 'itself',\n",
       " 'through',\n",
       " 'you',\n",
       " 'she',\n",
       " \"you'd\",\n",
       " 'there',\n",
       " 'too',\n",
       " 'just',\n",
       " 'yours',\n",
       " 'about',\n",
       " 'don',\n",
       " \"isn't\",\n",
       " 'is',\n",
       " 've',\n",
       " 'aren',\n",
       " 'dlss',\n",
       " \"wouldn't\",\n",
       " \"won't\",\n",
       " 'nt',\n",
       " 'does',\n",
       " \"couldn't\",\n",
       " 'games',\n",
       " 'couldn',\n",
       " 'himself',\n",
       " 'will',\n",
       " 'before',\n",
       " 'into',\n",
       " 'been',\n",
       " 'our',\n",
       " 'all',\n",
       " 'performance',\n",
       " 'some',\n",
       " 'with',\n",
       " 'them',\n",
       " 'by',\n",
       " 'these',\n",
       " 'any',\n",
       " \"shouldn't\",\n",
       " \"mightn't\",\n",
       " 's',\n",
       " 'card',\n",
       " 'haven',\n",
       " 'doing',\n",
       " 'now',\n",
       " 'better',\n",
       " 'have',\n",
       " 'during',\n",
       " 'until']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list_with_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook on Modelling, I will only be considering the \"text\" column of the scraped dataset, and this has been pre-processed in notebook 2. This ensures that our model can be properly trained on the content of the subreddit posts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline \n",
    "We always begin with creating a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_amd\n",
       "1    0.629734\n",
       "0    0.370266\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model\n",
    "X = df['text']\n",
    "y = df['is_amd']\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Steps I took for this section:\n",
    "1. Train Test Split\n",
    "2. Instantiating Vectorizers and Models\n",
    "3. Creating a User-Defined Function* with Scikit-learn's Pipeline tool that will help calculate the relevant classification metrics from each model (Metrics include Accuracy, Specificity and F1_Score)\n",
    "4. Evaluate best model\n",
    "5. Tune Hyper-parameters of best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48622,)\n",
      "(12156,)\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify = y)\n",
    "X_train = X_train.values.astype('U')\n",
    "X_test = X_test.values.astype('U')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words=stopwords_list)\n",
    "cvec.fit(X_train)\n",
    "X_train = cvec.transform(X_train) #transform the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaaaa' ... 'zz' 'zzx' 'zzzzz']\n",
      "(48622, 32966)\n"
     ]
    }
   ],
   "source": [
    "print(cvec.get_feature_names_out())\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test\n",
    "X_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify = y)\n",
    "X_train = X_train.values.astype('U')\n",
    "X_test = X_test.values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['must be something weird with nanite i m not sure to be honest maybe they are potato quality because they wanted consoles at fps and they didn t update them for pc i m just guessing though',\n",
       "       'deleted', 'the switch port doesnt have fsr', ...,\n",
       "       'no x either but can just look at the x for it i suppose since it s pretty much the same in gaming mostly',\n",
       "       'yeah it was weird what s even weirder is that none of the dx s advertised performance advantages ever materialised in regular games i guess that game developers decided to use the freed resources to make their games even less optimized',\n",
       "       'same here got mine in on friday to take the place of my and it s such a massive improvement i just wish fsr could remove jaggies and shinies as well as dlss does'],\n",
       "      dtype='<U6613')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Instantiation of Vectorizers and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. I will be exploring different Classification algorithms:\n",
    "- Multinomial Naive Bayes: Based on Bayes's theorem - the assumption that each feature (in our case, each word) is independent of each other.\n",
    "- Logistic Regression:\n",
    "- Random Forest: Consists of n number of decision trees that act as an ensemble. Each decision tree makes a class prediction and the class with the most votes becomes the model's prediction.  \n",
    "- K-nearest Neighbors Classifier  \n",
    "\n",
    "2.2. I will also be evaluating the models using both Count Vectorizer or Term Frequency-Inverse Document Frequency (TFIDF) transformers:\n",
    "- Count Vectorizer: Takes every word as a token, and uses it as a feature.\n",
    "- TFIFD: accounts for frequency of a word in a given document and the frequency between documents. Word importance increases proportionally to the number of times it appears in a document, but is offset by frequency of word in entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instiantiate models\n",
    "models = {'nb': MultinomialNB(),\n",
    "          'log_reg': LogisticRegression(max_iter=500, random_state=123),\n",
    "          'rf': RandomForestClassifier(random_state=123),\n",
    "          'knn': KNeighborsClassifier()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. User-Defined Function - inputs required are vectorizer and model\n",
    "\n",
    "3.1 Ngram Range option: To explore if different n-grams will give better results. I have opted to include this based on the EDA in Notebook 2 - seems like some bigrams and trigrams might be good predictors of whether or not a Subreddit is from AMD or Nvidia. <br>\n",
    "\n",
    "3.2 List of Stopwords: Explore if the standard stopword list in nltk of English words or English words + common words from both subreddits will produce better results. By removing them, I am retaining the words that are more specific to the content of each subreddit. This might enhance the model's ability to capture the distinctive language and topics associated with each community, hence improving model performance.\n",
    "\n",
    "3.3 Function will produce a dataframe of classification results: Accuracy, Specificity, F_Score, ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_results = []\n",
    "\n",
    "def clf_model(vec, mod, stopwords, ngram_range, cv_num,type):   # option to include Grid Search\n",
    "\n",
    "    vec_params={}\n",
    "    results = {}\n",
    "    vec_params['ngram_range'] = ngram_range  # Add ngram_range to the vectorizer parameters\n",
    "    vec_params['stop_words'] = stopwords\n",
    "    \n",
    "    # Instantiate Vectorizers\n",
    "    if vec == 'cvec':     \n",
    "        vectorizer = CountVectorizer(**vec_params)\n",
    "    elif vec == 'tvec':\n",
    "        vectorizer = TfidfVectorizer(**vec_params)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid 'vec' parameter. Supported values are 'cvec' and 'tvec'.\")\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "            (vec, vectorizer), \n",
    "            (mod, models[mod])\n",
    "            ])     # pipeline helps to automatically transform the data\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    preds = pipe.predict(X_test)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "    acc = (tp + tn)/ (tp+tn+fp+fn)\n",
    "    spec = tn / (tn + fp)\n",
    "    \n",
    "    # Retrieve metrics\n",
    "    results['Model'] = mod\n",
    "    results['Vectorizer'] = vec\n",
    "    results['Train Score'] = pipe.score(X_train, y_train)\n",
    "    results['Test Score'] = pipe.score(X_test, y_test)\n",
    "    results['Accuracy'] = acc\n",
    "    results['Specificity'] = spec\n",
    "    results['f_score'] = f1_score(y_test, preds)\n",
    "    results['ROC_AUC'] = roc_auc_score(y_test, preds)\n",
    "    results['Ngram Range'] = ngram_range\n",
    "    results['Stopword List'] = type\n",
    "        \n",
    "    df_results.append(results)\n",
    "    \n",
    "    print(f\"--- METRICS for {mod},{vec} ---\")\n",
    "    display(results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.8411418699354202,\n",
       " 'Test Score': 0.7221125370187562,\n",
       " 'Accuracy': 0.7221125370187562,\n",
       " 'Specificity': 0.5136636303043768,\n",
       " 'f_score': 0.7928877988963827,\n",
       " 'ROC_AUC': 0.6791701561058134,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.8489778289663116,\n",
       " 'Test Score': 0.7411977624218493,\n",
       " 'Accuracy': 0.7411977624218493,\n",
       " 'Specificity': 0.5509886691846256,\n",
       " 'f_score': 0.8058743675182032,\n",
       " 'ROC_AUC': 0.7020129498764408,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9760396528320513,\n",
       " 'Test Score': 0.7392234287594603,\n",
       " 'Accuracy': 0.7392234287594603,\n",
       " 'Specificity': 0.5314374583425905,\n",
       " 'f_score': 0.8062110282430616,\n",
       " 'ROC_AUC': 0.6964176187859262,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9793714779318005,\n",
       " 'Test Score': 0.7535373478117802,\n",
       " 'Accuracy': 0.7535373478117802,\n",
       " 'Specificity': 0.5660964230171073,\n",
       " 'f_score': 0.8152897657213317,\n",
       " 'ROC_AUC': 0.7149228032786384,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9790424087861462,\n",
       " 'Test Score': 0.7360974004606778,\n",
       " 'Accuracy': 0.7360974004606778,\n",
       " 'Specificity': 0.5132192846034215,\n",
       " 'f_score': 0.8053870419801019,\n",
       " 'ROC_AUC': 0.6901824705185624,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9830323721772037,\n",
       " 'Test Score': 0.7544422507403751,\n",
       " 'Accuracy': 0.7544422507403751,\n",
       " 'Specificity': 0.552766051988447,\n",
       " 'f_score': 0.8174423582655496,\n",
       " 'ROC_AUC': 0.71289510959971,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.7893957467812924,\n",
       " 'Test Score': 0.7321487331359,\n",
       " 'Accuracy': 0.7321487331359,\n",
       " 'Specificity': 0.48655854254610087,\n",
       " 'f_score': 0.804749340369393,\n",
       " 'ROC_AUC': 0.681554908111718,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8019003743161531,\n",
       " 'Test Score': 0.7492596248766041,\n",
       " 'Accuracy': 0.7492596248766041,\n",
       " 'Specificity': 0.5263274827816041,\n",
       " 'f_score': 0.8155633547137843,\n",
       " 'ROC_AUC': 0.7033335650354787,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8539344329727284,\n",
       " 'Test Score': 0.7319842053307009,\n",
       " 'Accuracy': 0.7319842053307009,\n",
       " 'Specificity': 0.4652299489002444,\n",
       " 'f_score': 0.8068303094983993,\n",
       " 'ROC_AUC': 0.6770303892117159,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8627987330837892,\n",
       " 'Test Score': 0.7524679170779862,\n",
       " 'Accuracy': 0.7524679170779862,\n",
       " 'Specificity': 0.5065540990890913,\n",
       " 'f_score': 0.8202831033864898,\n",
       " 'ROC_AUC': 0.7018074218502283,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8632512031590638,\n",
       " 'Test Score': 0.730174399473511,\n",
       " 'Accuracy': 0.730174399473511,\n",
       " 'Specificity': 0.4723394801155299,\n",
       " 'f_score': 0.8045292014302741,\n",
       " 'ROC_AUC': 0.6770580483529969,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for log_reg,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'log_reg',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8734523466743449,\n",
       " 'Test Score': 0.7476143468246134,\n",
       " 'Accuracy': 0.7476143468246134,\n",
       " 'Specificity': 0.505443234836703,\n",
       " 'f_score': 0.816221396909069,\n",
       " 'ROC_AUC': 0.6977248832576722,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;,\n",
       "                                             &#x27;but&#x27;, &#x27;only&#x27;, &#x27;they&#x27;, &#x27;am&#x27;,\n",
       "                                             &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;didn&#x27;, &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;,\n",
       "                                             &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;, &#x27;here&#x27;,\n",
       "                                             &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;,\n",
       "                                             &#x27;can&#x27;, &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;,\n",
       "                                             &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])),\n",
       "                (&#x27;log_reg&#x27;,\n",
       "                 LogisticRegression(max_iter=500, random_state=123))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;,\n",
       "                                             &#x27;but&#x27;, &#x27;only&#x27;, &#x27;they&#x27;, &#x27;am&#x27;,\n",
       "                                             &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;didn&#x27;, &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;,\n",
       "                                             &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;, &#x27;here&#x27;,\n",
       "                                             &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;,\n",
       "                                             &#x27;can&#x27;, &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;,\n",
       "                                             &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])),\n",
       "                (&#x27;log_reg&#x27;,\n",
       "                 LogisticRegression(max_iter=500, random_state=123))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 3),\n",
       "                stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;, &#x27;but&#x27;, &#x27;only&#x27;,\n",
       "                            &#x27;they&#x27;, &#x27;am&#x27;, &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;, &#x27;didn&#x27;,\n",
       "                            &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;, &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;,\n",
       "                            &#x27;here&#x27;, &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;, &#x27;can&#x27;,\n",
       "                            &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;, &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500, random_state=123)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=['through', 'be', 'nor', 'the',\n",
       "                                             'but', 'only', 'they', 'am',\n",
       "                                             'same', 'll', 'their', 'are',\n",
       "                                             'didn', 'my', 'after', 'some',\n",
       "                                             \"you'll\", 'just', 'been', 'here',\n",
       "                                             \"don't\", 'for', \"haven't\", 'at',\n",
       "                                             'can', \"hasn't\", 'both', \"shan't\",\n",
       "                                             'with', \"needn't\", ...])),\n",
       "                ('log_reg',\n",
       "                 LogisticRegression(max_iter=500, random_state=123))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "clf_model('cvec', 'log_reg', stopwords_list_with_common, (1,1), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'log_reg', stopwords_list, (1,1), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('cvec', 'log_reg', stopwords_list_with_common, (1,2), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'log_reg', stopwords_list, (1,2), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('cvec', 'log_reg', stopwords_list_with_common, (1,3), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'log_reg', stopwords_list, (1,3), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'log_reg', stopwords_list_with_common, (1,1), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'log_reg', stopwords_list, (1,1), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'log_reg', stopwords_list_with_common, (1,2), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'log_reg', stopwords_list, (1,2), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'log_reg', stopwords_list_with_common, (1,3), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'log_reg', stopwords_list, (1,3), 10 ,\"Stop Words List with English Words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.7873596314425568,\n",
       " 'Test Score': 0.7274596906877262,\n",
       " 'Accuracy': 0.7274596906877262,\n",
       " 'Specificity': 0.5734281270828705,\n",
       " 'f_score': 0.7908063395845174,\n",
       " 'ROC_AUC': 0.6957277800665822,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.7899099173213772,\n",
       " 'Test Score': 0.7362619282658769,\n",
       " 'Accuracy': 0.7362619282658769,\n",
       " 'Specificity': 0.6049766718506998,\n",
       " 'f_score': 0.7952745849297572,\n",
       " 'ROC_AUC': 0.7092159649260031,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9510098309407264,\n",
       " 'Test Score': 0.730174399473511,\n",
       " 'Accuracy': 0.730174399473511,\n",
       " 'Specificity': 0.39457898244834483,\n",
       " 'f_score': 0.8123569794050344,\n",
       " 'ROC_AUC': 0.6610386747643422,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9473695035169265,\n",
       " 'Test Score': 0.7431720960842383,\n",
       " 'Accuracy': 0.7431720960842383,\n",
       " 'Specificity': 0.44123528104865584,\n",
       " 'f_score': 0.8186781275409456,\n",
       " 'ROC_AUC': 0.6809703511709642,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9677306569042821,\n",
       " 'Test Score': 0.7205495228693649,\n",
       " 'Accuracy': 0.7205495228693649,\n",
       " 'Specificity': 0.32948233725838705,\n",
       " 'f_score': 0.8107415454899994,\n",
       " 'ROC_AUC': 0.6399861065782464,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9698490395294311,\n",
       " 'Test Score': 0.729104968739717,\n",
       " 'Accuracy': 0.729104968739717,\n",
       " 'Specificity': 0.3556987336147523,\n",
       " 'f_score': 0.8151765168097883,\n",
       " 'ROC_AUC': 0.6521798697466314,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.7480975690016864,\n",
       " 'Test Score': 0.6994076999012833,\n",
       " 'Accuracy': 0.6994076999012833,\n",
       " 'Specificity': 0.251721839591202,\n",
       " 'f_score': 0.801326663766855,\n",
       " 'ROC_AUC': 0.607180318881166,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.7526839702192423,\n",
       " 'Test Score': 0.7042612701546561,\n",
       " 'Accuracy': 0.7042612701546561,\n",
       " 'Specificity': 0.2646078649189069,\n",
       " 'f_score': 0.8039269157349331,\n",
       " 'ROC_AUC': 0.6136886483314326,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.7654353996133437,\n",
       " 'Test Score': 0.660167818361303,\n",
       " 'Accuracy': 0.660167818361303,\n",
       " 'Specificity': 0.09264607864918907,\n",
       " 'f_score': 0.7864785238021398,\n",
       " 'ROC_AUC': 0.5432531503631315,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.7652091645757064,\n",
       " 'Test Score': 0.6645278051990786,\n",
       " 'Accuracy': 0.6645278051990786,\n",
       " 'Specificity': 0.10619862252832704,\n",
       " 'f_score': 0.7884635335615727,\n",
       " 'ROC_AUC': 0.5495068880113876,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8152688083583562,\n",
       " 'Test Score': 0.6549029285949326,\n",
       " 'Accuracy': 0.6549029285949326,\n",
       " 'Specificity': 0.07598311486336369,\n",
       " 'f_score': 0.7841300879946483,\n",
       " 'ROC_AUC': 0.535640153120774,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for nb,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'nb',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.8101476697791123,\n",
       " 'Test Score': 0.6576999012833169,\n",
       " 'Accuracy': 0.6576999012833169,\n",
       " 'Specificity': 0.08353699177960454,\n",
       " 'f_score': 0.7855044074436827,\n",
       " 'ROC_AUC': 0.5394170915788943,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;,\n",
       "                                             &#x27;but&#x27;, &#x27;only&#x27;, &#x27;they&#x27;, &#x27;am&#x27;,\n",
       "                                             &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;didn&#x27;, &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;,\n",
       "                                             &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;, &#x27;here&#x27;,\n",
       "                                             &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;,\n",
       "                                             &#x27;can&#x27;, &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;,\n",
       "                                             &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])),\n",
       "                (&#x27;nb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;,\n",
       "                                             &#x27;but&#x27;, &#x27;only&#x27;, &#x27;they&#x27;, &#x27;am&#x27;,\n",
       "                                             &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;didn&#x27;, &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;,\n",
       "                                             &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;, &#x27;here&#x27;,\n",
       "                                             &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;,\n",
       "                                             &#x27;can&#x27;, &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;,\n",
       "                                             &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])),\n",
       "                (&#x27;nb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 3),\n",
       "                stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;, &#x27;but&#x27;, &#x27;only&#x27;,\n",
       "                            &#x27;they&#x27;, &#x27;am&#x27;, &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;, &#x27;didn&#x27;,\n",
       "                            &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;, &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;,\n",
       "                            &#x27;here&#x27;, &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;, &#x27;can&#x27;,\n",
       "                            &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;, &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=['through', 'be', 'nor', 'the',\n",
       "                                             'but', 'only', 'they', 'am',\n",
       "                                             'same', 'll', 'their', 'are',\n",
       "                                             'didn', 'my', 'after', 'some',\n",
       "                                             \"you'll\", 'just', 'been', 'here',\n",
       "                                             \"don't\", 'for', \"haven't\", 'at',\n",
       "                                             'can', \"hasn't\", 'both', \"shan't\",\n",
       "                                             'with', \"needn't\", ...])),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "clf_model('cvec', 'nb', stopwords_list_with_common, (1,1), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'nb', stopwords_list, (1,1), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('cvec', 'nb', stopwords_list_with_common, (1,2), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'nb', stopwords_list, (1,2), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('cvec', 'nb', stopwords_list_with_common, (1,3), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'nb', stopwords_list, (1,3), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'nb', stopwords_list_with_common, (1,1), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'nb', stopwords_list, (1,1), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'nb', stopwords_list_with_common, (1,2), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'nb', stopwords_list, (1,2), 10 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'nb', stopwords_list_with_common, (1,3), 10,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'nb', stopwords_list, (1,3), 10 ,\"Stop Words List with English Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for rf,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'rf',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9886059808317222,\n",
       " 'Test Score': 0.7185751892069759,\n",
       " 'Accuracy': 0.7185751892069759,\n",
       " 'Specificity': 0.5158853588091535,\n",
       " 'f_score': 0.7894380501015573,\n",
       " 'ROC_AUC': 0.6768192306782541,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for rf,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'rf',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9904981284192341,\n",
       " 'Test Score': 0.7348634419216847,\n",
       " 'Accuracy': 0.7348634419216847,\n",
       " 'Specificity': 0.5214396800710953,\n",
       " 'f_score': 0.8034156755108265,\n",
       " 'ROC_AUC': 0.6908961953588657,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for rf,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'rf',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9886882481181358,\n",
       " 'Test Score': 0.7123231326094109,\n",
       " 'Accuracy': 0.7123231326094109,\n",
       " 'Specificity': 0.4196845145523217,\n",
       " 'f_score': 0.7947408581323002,\n",
       " 'ROC_AUC': 0.652036901299675,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for rf,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'rf',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.9905803957056476,\n",
       " 'Test Score': 0.7335472194800922,\n",
       " 'Accuracy': 0.7335472194800922,\n",
       " 'Specificity': 0.44590091090868694,\n",
       " 'f_score': 0.8101295503839615,\n",
       " 'ROC_AUC': 0.6742894495758327,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;cvec&#x27;,\n",
       "                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                 stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;,\n",
       "                                             &#x27;but&#x27;, &#x27;only&#x27;, &#x27;they&#x27;, &#x27;am&#x27;,\n",
       "                                             &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;didn&#x27;, &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;,\n",
       "                                             &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;, &#x27;here&#x27;,\n",
       "                                             &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;,\n",
       "                                             &#x27;can&#x27;, &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;,\n",
       "                                             &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])),\n",
       "                (&#x27;rf&#x27;, RandomForestClassifier(random_state=123))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;cvec&#x27;,\n",
       "                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                 stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;,\n",
       "                                             &#x27;but&#x27;, &#x27;only&#x27;, &#x27;they&#x27;, &#x27;am&#x27;,\n",
       "                                             &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;didn&#x27;, &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;,\n",
       "                                             &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;, &#x27;here&#x27;,\n",
       "                                             &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;,\n",
       "                                             &#x27;can&#x27;, &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;,\n",
       "                                             &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])),\n",
       "                (&#x27;rf&#x27;, RandomForestClassifier(random_state=123))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;, &#x27;but&#x27;, &#x27;only&#x27;,\n",
       "                            &#x27;they&#x27;, &#x27;am&#x27;, &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;, &#x27;didn&#x27;,\n",
       "                            &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;, &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;,\n",
       "                            &#x27;here&#x27;, &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;, &#x27;can&#x27;,\n",
       "                            &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;, &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=123)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                 stop_words=['through', 'be', 'nor', 'the',\n",
       "                                             'but', 'only', 'they', 'am',\n",
       "                                             'same', 'll', 'their', 'are',\n",
       "                                             'didn', 'my', 'after', 'some',\n",
       "                                             \"you'll\", 'just', 'been', 'here',\n",
       "                                             \"don't\", 'for', \"haven't\", 'at',\n",
       "                                             'can', \"hasn't\", 'both', \"shan't\",\n",
       "                                             'with', \"needn't\", ...])),\n",
       "                ('rf', RandomForestClassifier(random_state=123))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "clf_model('cvec', 'rf', stopwords_list_with_common, (1,1), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'rf', stopwords_list, (1,1), 3 ,\"Stop Words List with English Words\")\n",
    "clf_model('cvec', 'rf', stopwords_list_with_common, (1,2), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'rf', stopwords_list, (1,2), 3 ,\"Stop Words List with English Words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.7567562008967135,\n",
       " 'Test Score': 0.6115498519249754,\n",
       " 'Accuracy': 0.6115498519249754,\n",
       " 'Specificity': 0.4630082203954677,\n",
       " 'f_score': 0.693814031902477,\n",
       " 'ROC_AUC': 0.5809489175132139,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.7674303813088725,\n",
       " 'Test Score': 0.638203356367226,\n",
       " 'Accuracy': 0.638203356367226,\n",
       " 'Specificity': 0.46167518329260165,\n",
       " 'f_score': 0.7209036679781698,\n",
       " 'ROC_AUC': 0.6018369384784367,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.747645098926412,\n",
       " 'Test Score': 0.607683448502797,\n",
       " 'Accuracy': 0.607683448502797,\n",
       " 'Specificity': 0.4463452566096423,\n",
       " 'f_score': 0.692818035426731,\n",
       " 'ROC_AUC': 0.5744463056398963,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.7602525605692896,\n",
       " 'Test Score': 0.6284139519578809,\n",
       " 'Accuracy': 0.6284139519578809,\n",
       " 'Specificity': 0.4456787380582093,\n",
       " 'f_score': 0.7138059937907876,\n",
       " 'ROC_AUC': 0.5907688268997774,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.7419069556990663,\n",
       " 'Test Score': 0.5966600855544587,\n",
       " 'Accuracy': 0.5966600855544587,\n",
       " 'Specificity': 0.4452343923572539,\n",
       " 'f_score': 0.6816440490877217,\n",
       " 'ROC_AUC': 0.5654650080662821,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,cvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'cvec',\n",
       " 'Train Score': 0.7531775739377237,\n",
       " 'Test Score': 0.6280026324448832,\n",
       " 'Accuracy': 0.6280026324448832,\n",
       " 'Specificity': 0.43034881137524994,\n",
       " 'f_score': 0.7158833877858759,\n",
       " 'ROC_AUC': 0.5872841378888007,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.6689153058286372,\n",
       " 'Test Score': 0.644290885159592,\n",
       " 'Accuracy': 0.644290885159592,\n",
       " 'Specificity': 0.09753388135969784,\n",
       " 'f_score': 0.7737310308738881,\n",
       " 'ROC_AUC': 0.5316539426393525,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.6847517584632471,\n",
       " 'Test Score': 0.6412471207634091,\n",
       " 'Accuracy': 0.6412471207634091,\n",
       " 'Specificity': 0.08842479449011331,\n",
       " 'f_score': 0.7723309840772644,\n",
       " 'ROC_AUC': 0.5273606663502166,\n",
       " 'Ngram Range': (1, 1),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.6379622393155362,\n",
       " 'Test Score': 0.6336788417242514,\n",
       " 'Accuracy': 0.6336788417242514,\n",
       " 'Specificity': 0.026438569206842923,\n",
       " 'f_score': 0.7730492839304828,\n",
       " 'ROC_AUC': 0.5085817927680198,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.644502488585414,\n",
       " 'Test Score': 0.6325271470878578,\n",
       " 'Accuracy': 0.6325271470878578,\n",
       " 'Specificity': 0.018440346589646744,\n",
       " 'f_score': 0.7730067584734996,\n",
       " 'ROC_AUC': 0.506019650760532,\n",
       " 'Ngram Range': (1, 2),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.6357204557607667,\n",
       " 'Test Score': 0.6322803553800592,\n",
       " 'Accuracy': 0.6322803553800592,\n",
       " 'Specificity': 0.02066207509442346,\n",
       " 'f_score': 0.7725885225885226,\n",
       " 'ROC_AUC': 0.506281396789537,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with Common Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- METRICS for knn,tvec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'knn',\n",
       " 'Vectorizer': 'tvec',\n",
       " 'Train Score': 0.6401423224054954,\n",
       " 'Test Score': 0.631951299769661,\n",
       " 'Accuracy': 0.631951299769661,\n",
       " 'Specificity': 0.015329926682959343,\n",
       " 'f_score': 0.7728934010152285,\n",
       " 'ROC_AUC': 0.5049216583120871,\n",
       " 'Ngram Range': (1, 3),\n",
       " 'Stopword List': 'Stop Words List with English Words'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;,\n",
       "                                             &#x27;but&#x27;, &#x27;only&#x27;, &#x27;they&#x27;, &#x27;am&#x27;,\n",
       "                                             &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;didn&#x27;, &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;,\n",
       "                                             &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;, &#x27;here&#x27;,\n",
       "                                             &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;,\n",
       "                                             &#x27;can&#x27;, &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;,\n",
       "                                             &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])),\n",
       "                (&#x27;knn&#x27;, KNeighborsClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;,\n",
       "                                             &#x27;but&#x27;, &#x27;only&#x27;, &#x27;they&#x27;, &#x27;am&#x27;,\n",
       "                                             &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;didn&#x27;, &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;,\n",
       "                                             &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;, &#x27;here&#x27;,\n",
       "                                             &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;,\n",
       "                                             &#x27;can&#x27;, &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;,\n",
       "                                             &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])),\n",
       "                (&#x27;knn&#x27;, KNeighborsClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 3),\n",
       "                stop_words=[&#x27;through&#x27;, &#x27;be&#x27;, &#x27;nor&#x27;, &#x27;the&#x27;, &#x27;but&#x27;, &#x27;only&#x27;,\n",
       "                            &#x27;they&#x27;, &#x27;am&#x27;, &#x27;same&#x27;, &#x27;ll&#x27;, &#x27;their&#x27;, &#x27;are&#x27;, &#x27;didn&#x27;,\n",
       "                            &#x27;my&#x27;, &#x27;after&#x27;, &#x27;some&#x27;, &quot;you&#x27;ll&quot;, &#x27;just&#x27;, &#x27;been&#x27;,\n",
       "                            &#x27;here&#x27;, &quot;don&#x27;t&quot;, &#x27;for&#x27;, &quot;haven&#x27;t&quot;, &#x27;at&#x27;, &#x27;can&#x27;,\n",
       "                            &quot;hasn&#x27;t&quot;, &#x27;both&#x27;, &quot;shan&#x27;t&quot;, &#x27;with&#x27;, &quot;needn&#x27;t&quot;, ...])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(ngram_range=(1, 3),\n",
       "                                 stop_words=['through', 'be', 'nor', 'the',\n",
       "                                             'but', 'only', 'they', 'am',\n",
       "                                             'same', 'll', 'their', 'are',\n",
       "                                             'didn', 'my', 'after', 'some',\n",
       "                                             \"you'll\", 'just', 'been', 'here',\n",
       "                                             \"don't\", 'for', \"haven't\", 'at',\n",
       "                                             'can', \"hasn't\", 'both', \"shan't\",\n",
       "                                             'with', \"needn't\", ...])),\n",
       "                ('knn', KNeighborsClassifier())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "clf_model('cvec', 'knn', stopwords_list_with_common, (1,1), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'knn', stopwords_list, (1,1), 3 ,\"Stop Words List with English Words\")\n",
    "clf_model('cvec', 'knn', stopwords_list_with_common, (1,2), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'knn', stopwords_list, (1,2), 3 ,\"Stop Words List with English Words\")\n",
    "clf_model('cvec', 'knn', stopwords_list_with_common, (1,3), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('cvec', 'knn', stopwords_list, (1,3), 3 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'knn', stopwords_list_with_common, (1,1), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'knn', stopwords_list, (1,1), 3 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'knn', stopwords_list_with_common, (1,2), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'knn', stopwords_list, (1,2), 3 ,\"Stop Words List with English Words\")\n",
    "clf_model('tvec', 'knn', stopwords_list_with_common, (1,3), 3,\"Stop Words List with Common Words\")\n",
    "clf_model('tvec', 'knn', stopwords_list, (1,3), 3 ,\"Stop Words List with English Words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>f_score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Ngram Range</th>\n",
       "      <th>Stopword List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.862799</td>\n",
       "      <td>0.752468</td>\n",
       "      <td>0.752468</td>\n",
       "      <td>0.506554</td>\n",
       "      <td>0.820283</td>\n",
       "      <td>0.701807</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.947370</td>\n",
       "      <td>0.743172</td>\n",
       "      <td>0.743172</td>\n",
       "      <td>0.441235</td>\n",
       "      <td>0.818678</td>\n",
       "      <td>0.680970</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.983032</td>\n",
       "      <td>0.754442</td>\n",
       "      <td>0.754442</td>\n",
       "      <td>0.552766</td>\n",
       "      <td>0.817442</td>\n",
       "      <td>0.712895</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.873452</td>\n",
       "      <td>0.747614</td>\n",
       "      <td>0.747614</td>\n",
       "      <td>0.505443</td>\n",
       "      <td>0.816221</td>\n",
       "      <td>0.697725</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.749260</td>\n",
       "      <td>0.749260</td>\n",
       "      <td>0.526327</td>\n",
       "      <td>0.815563</td>\n",
       "      <td>0.703334</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.979371</td>\n",
       "      <td>0.753537</td>\n",
       "      <td>0.753537</td>\n",
       "      <td>0.566096</td>\n",
       "      <td>0.815290</td>\n",
       "      <td>0.714923</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.969849</td>\n",
       "      <td>0.729105</td>\n",
       "      <td>0.729105</td>\n",
       "      <td>0.355699</td>\n",
       "      <td>0.815177</td>\n",
       "      <td>0.652180</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.951010</td>\n",
       "      <td>0.730174</td>\n",
       "      <td>0.730174</td>\n",
       "      <td>0.394579</td>\n",
       "      <td>0.812357</td>\n",
       "      <td>0.661039</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.967731</td>\n",
       "      <td>0.720550</td>\n",
       "      <td>0.720550</td>\n",
       "      <td>0.329482</td>\n",
       "      <td>0.810742</td>\n",
       "      <td>0.639986</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rf</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.990580</td>\n",
       "      <td>0.733547</td>\n",
       "      <td>0.733547</td>\n",
       "      <td>0.445901</td>\n",
       "      <td>0.810130</td>\n",
       "      <td>0.674289</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.853934</td>\n",
       "      <td>0.731984</td>\n",
       "      <td>0.731984</td>\n",
       "      <td>0.465230</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.677030</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.976040</td>\n",
       "      <td>0.739223</td>\n",
       "      <td>0.739223</td>\n",
       "      <td>0.531437</td>\n",
       "      <td>0.806211</td>\n",
       "      <td>0.696418</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.848978</td>\n",
       "      <td>0.741198</td>\n",
       "      <td>0.741198</td>\n",
       "      <td>0.550989</td>\n",
       "      <td>0.805874</td>\n",
       "      <td>0.702013</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.979042</td>\n",
       "      <td>0.736097</td>\n",
       "      <td>0.736097</td>\n",
       "      <td>0.513219</td>\n",
       "      <td>0.805387</td>\n",
       "      <td>0.690182</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.789396</td>\n",
       "      <td>0.732149</td>\n",
       "      <td>0.732149</td>\n",
       "      <td>0.486559</td>\n",
       "      <td>0.804749</td>\n",
       "      <td>0.681555</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.863251</td>\n",
       "      <td>0.730174</td>\n",
       "      <td>0.730174</td>\n",
       "      <td>0.472339</td>\n",
       "      <td>0.804529</td>\n",
       "      <td>0.677058</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.752684</td>\n",
       "      <td>0.704261</td>\n",
       "      <td>0.704261</td>\n",
       "      <td>0.264608</td>\n",
       "      <td>0.803927</td>\n",
       "      <td>0.613689</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rf</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.990498</td>\n",
       "      <td>0.734863</td>\n",
       "      <td>0.734863</td>\n",
       "      <td>0.521440</td>\n",
       "      <td>0.803416</td>\n",
       "      <td>0.690896</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.748098</td>\n",
       "      <td>0.699408</td>\n",
       "      <td>0.699408</td>\n",
       "      <td>0.251722</td>\n",
       "      <td>0.801327</td>\n",
       "      <td>0.607180</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.789910</td>\n",
       "      <td>0.736262</td>\n",
       "      <td>0.736262</td>\n",
       "      <td>0.604977</td>\n",
       "      <td>0.795275</td>\n",
       "      <td>0.709216</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rf</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.988688</td>\n",
       "      <td>0.712323</td>\n",
       "      <td>0.712323</td>\n",
       "      <td>0.419685</td>\n",
       "      <td>0.794741</td>\n",
       "      <td>0.652037</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log_reg</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.841142</td>\n",
       "      <td>0.722113</td>\n",
       "      <td>0.722113</td>\n",
       "      <td>0.513664</td>\n",
       "      <td>0.792888</td>\n",
       "      <td>0.679170</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.787360</td>\n",
       "      <td>0.727460</td>\n",
       "      <td>0.727460</td>\n",
       "      <td>0.573428</td>\n",
       "      <td>0.790806</td>\n",
       "      <td>0.695728</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rf</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.988606</td>\n",
       "      <td>0.718575</td>\n",
       "      <td>0.718575</td>\n",
       "      <td>0.515885</td>\n",
       "      <td>0.789438</td>\n",
       "      <td>0.676819</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.765209</td>\n",
       "      <td>0.664528</td>\n",
       "      <td>0.664528</td>\n",
       "      <td>0.106199</td>\n",
       "      <td>0.788464</td>\n",
       "      <td>0.549507</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.765435</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.092646</td>\n",
       "      <td>0.786479</td>\n",
       "      <td>0.543253</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.810148</td>\n",
       "      <td>0.657700</td>\n",
       "      <td>0.657700</td>\n",
       "      <td>0.083537</td>\n",
       "      <td>0.785504</td>\n",
       "      <td>0.539417</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.815269</td>\n",
       "      <td>0.654903</td>\n",
       "      <td>0.654903</td>\n",
       "      <td>0.075983</td>\n",
       "      <td>0.784130</td>\n",
       "      <td>0.535640</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>knn</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.668915</td>\n",
       "      <td>0.644291</td>\n",
       "      <td>0.644291</td>\n",
       "      <td>0.097534</td>\n",
       "      <td>0.773731</td>\n",
       "      <td>0.531654</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>knn</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.637962</td>\n",
       "      <td>0.633679</td>\n",
       "      <td>0.633679</td>\n",
       "      <td>0.026439</td>\n",
       "      <td>0.773049</td>\n",
       "      <td>0.508582</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>knn</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.644502</td>\n",
       "      <td>0.632527</td>\n",
       "      <td>0.632527</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.773007</td>\n",
       "      <td>0.506020</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>knn</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.640142</td>\n",
       "      <td>0.631951</td>\n",
       "      <td>0.631951</td>\n",
       "      <td>0.015330</td>\n",
       "      <td>0.772893</td>\n",
       "      <td>0.504922</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>knn</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.635720</td>\n",
       "      <td>0.632280</td>\n",
       "      <td>0.632280</td>\n",
       "      <td>0.020662</td>\n",
       "      <td>0.772589</td>\n",
       "      <td>0.506281</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>knn</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.684752</td>\n",
       "      <td>0.641247</td>\n",
       "      <td>0.641247</td>\n",
       "      <td>0.088425</td>\n",
       "      <td>0.772331</td>\n",
       "      <td>0.527361</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>knn</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.767430</td>\n",
       "      <td>0.638203</td>\n",
       "      <td>0.638203</td>\n",
       "      <td>0.461675</td>\n",
       "      <td>0.720904</td>\n",
       "      <td>0.601837</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>knn</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.753178</td>\n",
       "      <td>0.628003</td>\n",
       "      <td>0.628003</td>\n",
       "      <td>0.430349</td>\n",
       "      <td>0.715883</td>\n",
       "      <td>0.587284</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>knn</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.760253</td>\n",
       "      <td>0.628414</td>\n",
       "      <td>0.628414</td>\n",
       "      <td>0.445679</td>\n",
       "      <td>0.713806</td>\n",
       "      <td>0.590769</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>knn</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.756756</td>\n",
       "      <td>0.611550</td>\n",
       "      <td>0.611550</td>\n",
       "      <td>0.463008</td>\n",
       "      <td>0.693814</td>\n",
       "      <td>0.580949</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>knn</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.747645</td>\n",
       "      <td>0.607683</td>\n",
       "      <td>0.607683</td>\n",
       "      <td>0.446345</td>\n",
       "      <td>0.692818</td>\n",
       "      <td>0.574446</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>knn</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.741907</td>\n",
       "      <td>0.596660</td>\n",
       "      <td>0.596660</td>\n",
       "      <td>0.445234</td>\n",
       "      <td>0.681644</td>\n",
       "      <td>0.565465</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Stop Words List with Common Words</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model Vectorizer  Train Score  Test Score  Accuracy  Specificity  \\\n",
       "9   log_reg       tvec     0.862799    0.752468  0.752468     0.506554   \n",
       "15       nb       cvec     0.947370    0.743172  0.743172     0.441235   \n",
       "5   log_reg       cvec     0.983032    0.754442  0.754442     0.552766   \n",
       "11  log_reg       tvec     0.873452    0.747614  0.747614     0.505443   \n",
       "7   log_reg       tvec     0.801900    0.749260  0.749260     0.526327   \n",
       "3   log_reg       cvec     0.979371    0.753537  0.753537     0.566096   \n",
       "17       nb       cvec     0.969849    0.729105  0.729105     0.355699   \n",
       "14       nb       cvec     0.951010    0.730174  0.730174     0.394579   \n",
       "16       nb       cvec     0.967731    0.720550  0.720550     0.329482   \n",
       "27       rf       cvec     0.990580    0.733547  0.733547     0.445901   \n",
       "8   log_reg       tvec     0.853934    0.731984  0.731984     0.465230   \n",
       "2   log_reg       cvec     0.976040    0.739223  0.739223     0.531437   \n",
       "1   log_reg       cvec     0.848978    0.741198  0.741198     0.550989   \n",
       "4   log_reg       cvec     0.979042    0.736097  0.736097     0.513219   \n",
       "6   log_reg       tvec     0.789396    0.732149  0.732149     0.486559   \n",
       "10  log_reg       tvec     0.863251    0.730174  0.730174     0.472339   \n",
       "19       nb       tvec     0.752684    0.704261  0.704261     0.264608   \n",
       "25       rf       cvec     0.990498    0.734863  0.734863     0.521440   \n",
       "18       nb       tvec     0.748098    0.699408  0.699408     0.251722   \n",
       "13       nb       cvec     0.789910    0.736262  0.736262     0.604977   \n",
       "26       rf       cvec     0.988688    0.712323  0.712323     0.419685   \n",
       "0   log_reg       cvec     0.841142    0.722113  0.722113     0.513664   \n",
       "12       nb       cvec     0.787360    0.727460  0.727460     0.573428   \n",
       "24       rf       cvec     0.988606    0.718575  0.718575     0.515885   \n",
       "21       nb       tvec     0.765209    0.664528  0.664528     0.106199   \n",
       "20       nb       tvec     0.765435    0.660168  0.660168     0.092646   \n",
       "23       nb       tvec     0.810148    0.657700  0.657700     0.083537   \n",
       "22       nb       tvec     0.815269    0.654903  0.654903     0.075983   \n",
       "34      knn       tvec     0.668915    0.644291  0.644291     0.097534   \n",
       "36      knn       tvec     0.637962    0.633679  0.633679     0.026439   \n",
       "37      knn       tvec     0.644502    0.632527  0.632527     0.018440   \n",
       "39      knn       tvec     0.640142    0.631951  0.631951     0.015330   \n",
       "38      knn       tvec     0.635720    0.632280  0.632280     0.020662   \n",
       "35      knn       tvec     0.684752    0.641247  0.641247     0.088425   \n",
       "29      knn       cvec     0.767430    0.638203  0.638203     0.461675   \n",
       "33      knn       cvec     0.753178    0.628003  0.628003     0.430349   \n",
       "31      knn       cvec     0.760253    0.628414  0.628414     0.445679   \n",
       "28      knn       cvec     0.756756    0.611550  0.611550     0.463008   \n",
       "30      knn       cvec     0.747645    0.607683  0.607683     0.446345   \n",
       "32      knn       cvec     0.741907    0.596660  0.596660     0.445234   \n",
       "\n",
       "     f_score   ROC_AUC Ngram Range                       Stopword List  \n",
       "9   0.820283  0.701807      (1, 2)  Stop Words List with English Words  \n",
       "15  0.818678  0.680970      (1, 2)  Stop Words List with English Words  \n",
       "5   0.817442  0.712895      (1, 3)  Stop Words List with English Words  \n",
       "11  0.816221  0.697725      (1, 3)  Stop Words List with English Words  \n",
       "7   0.815563  0.703334      (1, 1)  Stop Words List with English Words  \n",
       "3   0.815290  0.714923      (1, 2)  Stop Words List with English Words  \n",
       "17  0.815177  0.652180      (1, 3)  Stop Words List with English Words  \n",
       "14  0.812357  0.661039      (1, 2)   Stop Words List with Common Words  \n",
       "16  0.810742  0.639986      (1, 3)   Stop Words List with Common Words  \n",
       "27  0.810130  0.674289      (1, 2)  Stop Words List with English Words  \n",
       "8   0.806830  0.677030      (1, 2)   Stop Words List with Common Words  \n",
       "2   0.806211  0.696418      (1, 2)   Stop Words List with Common Words  \n",
       "1   0.805874  0.702013      (1, 1)  Stop Words List with English Words  \n",
       "4   0.805387  0.690182      (1, 3)   Stop Words List with Common Words  \n",
       "6   0.804749  0.681555      (1, 1)   Stop Words List with Common Words  \n",
       "10  0.804529  0.677058      (1, 3)   Stop Words List with Common Words  \n",
       "19  0.803927  0.613689      (1, 1)  Stop Words List with English Words  \n",
       "25  0.803416  0.690896      (1, 1)  Stop Words List with English Words  \n",
       "18  0.801327  0.607180      (1, 1)   Stop Words List with Common Words  \n",
       "13  0.795275  0.709216      (1, 1)  Stop Words List with English Words  \n",
       "26  0.794741  0.652037      (1, 2)   Stop Words List with Common Words  \n",
       "0   0.792888  0.679170      (1, 1)   Stop Words List with Common Words  \n",
       "12  0.790806  0.695728      (1, 1)   Stop Words List with Common Words  \n",
       "24  0.789438  0.676819      (1, 1)   Stop Words List with Common Words  \n",
       "21  0.788464  0.549507      (1, 2)  Stop Words List with English Words  \n",
       "20  0.786479  0.543253      (1, 2)   Stop Words List with Common Words  \n",
       "23  0.785504  0.539417      (1, 3)  Stop Words List with English Words  \n",
       "22  0.784130  0.535640      (1, 3)   Stop Words List with Common Words  \n",
       "34  0.773731  0.531654      (1, 1)   Stop Words List with Common Words  \n",
       "36  0.773049  0.508582      (1, 2)   Stop Words List with Common Words  \n",
       "37  0.773007  0.506020      (1, 2)  Stop Words List with English Words  \n",
       "39  0.772893  0.504922      (1, 3)  Stop Words List with English Words  \n",
       "38  0.772589  0.506281      (1, 3)   Stop Words List with Common Words  \n",
       "35  0.772331  0.527361      (1, 1)  Stop Words List with English Words  \n",
       "29  0.720904  0.601837      (1, 1)  Stop Words List with English Words  \n",
       "33  0.715883  0.587284      (1, 3)  Stop Words List with English Words  \n",
       "31  0.713806  0.590769      (1, 2)  Stop Words List with English Words  \n",
       "28  0.693814  0.580949      (1, 1)   Stop Words List with Common Words  \n",
       "30  0.692818  0.574446      (1, 2)   Stop Words List with Common Words  \n",
       "32  0.681644  0.565465      (1, 3)   Stop Words List with Common Words  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dataframe of results\n",
    "df_results = pd.DataFrame(df_results)\n",
    "df_results_sorted = pd.DataFrame(df_results).sort_values(by='f_score', ascending=False)\n",
    "df_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Hyperparameter Tuning of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a robust machine learning model, we need to select the optimal combination of hyperparameters. Depending on the type of vectorizer and classification model, there will be a different set of hyperparameters. It is not practical to perform hyperparameter tuning on all combination of models and vectorizers due to the complexity and hence time taken. Hence, I will select the best models from Section 2.1 and perform HalvingGridSearchCV.\n",
    "GridsearchCV searches all combinations of parameters in for a model that will give the best performance score, while HalvingGridSearchCV will be used on the Random Forest model to reduce run time. I have opted for the latter as it is time and computationally efficient.\n",
    "\n",
    "From the above results table in 2.1, we can see that the best performing models based on F1 score* are:\n",
    "- Multinomial Naive Bayes Model (CVEC, bigram)\n",
    "- Logistic Regression (CVEC and TVEC, bigrams and trigrams) \n",
    "- Random Forest perform (CVEC and TVEC)\n",
    "\n",
    "*F1 score was selected as the evaluation metric of model performance as it helps to balance between mnimising false positives (precision) and false negatives (recall). <br>\n",
    "I will proceed to tune these models' hyperparameters to get the best combination. From there, we can determine which model performs the best at this classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to easily generate and compare the results, we will creating a Function to perform GridSearchCV or HalvingGridSearchCV, with inputs being parameter grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters \n",
    "\n",
    "tuning_results = []\n",
    "\n",
    "#cvec_nb_params \n",
    "\n",
    "# CVEC and Logistic Regression\n",
    "cvec_logr_params = {'cvec__max_features': [5000, 6000, 7000, None],\n",
    "                    'cvec__max_df': [0.5, 0.75, 0.9, 1],\n",
    "                    'cvec__min_df': [1, 2, 3],\n",
    "                    'cvec__ngram_range': [(1,2), (1,3)],   # from earlier results table, bigrams and trigram perform better\n",
    "                    'log_reg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                    'log_reg__C': [1, 0.5, 0.1] }\n",
    "\n",
    "# TFIFD and Logistic Regression\n",
    "\n",
    "tvec_logr_params = {'tvec__max_features': [5000, 6000, 7000, None],\n",
    "                    'tvec__max_df': [0.5, 0.75, 0.9, 1],\n",
    "                    'tvec__min_df': [1, 2, 3],\n",
    "                    'tvec__ngram_range': [(1,2), (1,3)],   # from earlier results table, bigrams and trigram perform better\n",
    "                    'log_reg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                    'log_reg__C': [1, 0.5, 0.1] }\n",
    "\n",
    "\n",
    "\n",
    "# CVEC and Random Forest\n",
    "\n",
    "rf_cvec_params = {\n",
    "    'cvec__max_features':  [5000, 6000, 7000, None],\n",
    "    'cvec__max_df':[0.5, 0.75, 0.9, 1],\n",
    "    'cvec__min_df': [1, 2, 3],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [50,75,100]}\n",
    "\n",
    "# TFIFD and Random Forest\n",
    "\n",
    "rf_tvec_params = {\n",
    "    'tvec__max_features':  [5000, 6000, 7000, None],\n",
    "    'tvec__max_df': [0.5, 0.75, 0.9, 1],\n",
    "    'tvec__min_df': [1, 2, 3],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [50,75,100]}\n",
    "\n",
    "# TFIFD and NB\n",
    "\n",
    "nb_tvec_params = {\n",
    "    'tvec__max_features':  [5000, 6000, 7000, None],\n",
    "    'tvec__max_df': [0.5, 0.75, 0.9, 1],\n",
    "    'tvec__min_df': [1, 2, 3],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'nb__alpha': np.linspace(0, 1, 5),\n",
    "    'nb__fit_prior': [True, False]\n",
    "    }\n",
    "\n",
    "# CVEC and NB\n",
    "\n",
    "nb_cvec_params = {\n",
    "    'cvec__max_features': [5000, 6000, 7000, None],\n",
    "    'cvec__min_df':[0.5, 0.75, 0.9, 1],\n",
    "    'cvec__max_df': [0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    'cvec__ngram_range':[(1,1), (1,2)],\n",
    "    'nb__alpha': np.linspace(0, 1, 5),\n",
    "    'nb__fit_prior': [True, False]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_tuning_results = []\n",
    "\n",
    "def perform_grid_search(X_train, y_train, vec, mod, param_grid, cv=10):\n",
    " \n",
    "    if vec == 'cvec':\n",
    "        vectorizer = CountVectorizer(stop_words=stopwords_list)\n",
    "    elif vec == 'tvec':\n",
    "        vectorizer = TfidfVectorizer(stop_words=stopwords_list)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid 'vec' parameter. Supported values are 'cvec' and 'tvec'.\")\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        (vec, vectorizer),\n",
    "        (mod, models[mod])\n",
    "    ])\n",
    "\n",
    "    # Perform HalvingGridSearch\n",
    "\n",
    "    gs = HalvingGridSearchCV(pipe, param_grid=param_grid, cv=cv, n_jobs=-1, verbose=1)\n",
    "    gs.fit(X_train, y_train)\n",
    "  \n",
    "    # Get the best model and best parameters\n",
    "    best_estimator = gs.best_estimator_\n",
    "    best_params = gs.best_params_\n",
    "\n",
    "    # Predict on test set\n",
    "    preds = best_estimator.predict(X_test)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "    acc = (tp + tn)/ (tp+tn+fp+fn)\n",
    "    spec = tn / (tn + fp)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    train_score = best_estimator.score(X_train, y_train)\n",
    "    test_score = best_estimator.score(X_test, y_test)\n",
    "    f_score = f1_score(y_test, preds)\n",
    "    roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    # Append the results to df_tuning_results list\n",
    "    result_dict = {\n",
    "        'Vectorizer': vec,\n",
    "        'Model': mod,\n",
    "        'Best_Estimator': best_estimator,\n",
    "        'Best_Params': best_params,\n",
    "        'Train Score': train_score,\n",
    "        'Test Score': test_score,\n",
    "        'Accuracy': acc,\n",
    "        'Specificity': spec,\n",
    "        'f_score': f_score,\n",
    "        'ROC_AUC': roc_auc,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df_tuning_results.append(result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 66\n",
      "max_resources_: 48622\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 864\n",
      "n_resources: 66\n",
      "Fitting 10 folds for each of 864 candidates, totalling 8640 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 288\n",
      "n_resources: 198\n",
      "Fitting 10 folds for each of 288 candidates, totalling 2880 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 96\n",
      "n_resources: 594\n",
      "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 32\n",
      "n_resources: 1782\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 11\n",
      "n_resources: 5346\n",
      "Fitting 10 folds for each of 11 candidates, totalling 110 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 4\n",
      "n_resources: 16038\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 2\n",
      "n_resources: 48114\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 66\n",
      "max_resources_: 48622\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 864\n",
      "n_resources: 66\n",
      "Fitting 10 folds for each of 864 candidates, totalling 8640 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 288\n",
      "n_resources: 198\n",
      "Fitting 10 folds for each of 288 candidates, totalling 2880 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 96\n",
      "n_resources: 594\n",
      "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 32\n",
      "n_resources: 1782\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 11\n",
      "n_resources: 5346\n",
      "Fitting 10 folds for each of 11 candidates, totalling 110 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 4\n",
      "n_resources: 16038\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 2\n",
      "n_resources: 48114\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Logistic Regression\n",
    "perform_grid_search(X_train, y_train, 'cvec', 'log_reg', cvec_logr_params)\n",
    "perform_grid_search(X_train, y_train, 'tvec', 'log_reg', tvec_logr_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 5\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 200\n",
      "max_resources_: 48622\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 288\n",
      "n_resources: 200\n",
      "Fitting 10 folds for each of 288 candidates, totalling 2880 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 96\n",
      "n_resources: 600\n",
      "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 32\n",
      "n_resources: 1800\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 11\n",
      "n_resources: 5400\n",
      "Fitting 10 folds for each of 11 candidates, totalling 110 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 16200\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "n_iterations: 5\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 200\n",
      "max_resources_: 48622\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 288\n",
      "n_resources: 200\n",
      "Fitting 10 folds for each of 288 candidates, totalling 2880 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 96\n",
      "n_resources: 600\n",
      "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 32\n",
      "n_resources: 1800\n",
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 11\n",
      "n_resources: 5400\n",
      "Fitting 10 folds for each of 11 candidates, totalling 110 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 16200\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "perform_grid_search(X_train, y_train, 'cvec', 'rf', rf_cvec_params)\n",
    "perform_grid_search(X_train, y_train, 'tvec', 'rf', rf_tvec_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 66\n",
      "max_resources_: 48622\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1600\n",
      "n_resources: 66\n",
      "Fitting 10 folds for each of 1600 candidates, totalling 16000 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 534\n",
      "n_resources: 198\n",
      "Fitting 10 folds for each of 534 candidates, totalling 5340 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 178\n",
      "n_resources: 594\n",
      "Fitting 10 folds for each of 178 candidates, totalling 1780 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 60\n",
      "n_resources: 1782\n",
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 20\n",
      "n_resources: 5346\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 7\n",
      "n_resources: 16038\n",
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 3\n",
      "n_resources: 48114\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 66\n",
      "max_resources_: 48622\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 960\n",
      "n_resources: 66\n",
      "Fitting 10 folds for each of 960 candidates, totalling 9600 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 320\n",
      "n_resources: 198\n",
      "Fitting 10 folds for each of 320 candidates, totalling 3200 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 107\n",
      "n_resources: 594\n",
      "Fitting 10 folds for each of 107 candidates, totalling 1070 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 36\n",
      "n_resources: 1782\n",
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 12\n",
      "n_resources: 5346\n",
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 4\n",
      "n_resources: 16038\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 2\n",
      "n_resources: 48114\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "# Multinominal Naive Bayes\n",
    "\n",
    "perform_grid_search(X_train, y_train, 'cvec', 'nb', nb_cvec_params)\n",
    "perform_grid_search(X_train, y_train, 'tvec', 'nb', nb_tvec_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>Best_Estimator</th>\n",
       "      <th>Best_Params</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>f_score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec</td>\n",
       "      <td>log_reg</td>\n",
       "      <td>(CountVectorizer(max_df=0.5, ngram_range=(1, 3...</td>\n",
       "      <td>{'cvec__max_df': 0.5, 'cvec__max_features': No...</td>\n",
       "      <td>0.975711</td>\n",
       "      <td>0.753620</td>\n",
       "      <td>0.753620</td>\n",
       "      <td>0.538991</td>\n",
       "      <td>0.818099</td>\n",
       "      <td>0.709404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tvec</td>\n",
       "      <td>log_reg</td>\n",
       "      <td>(TfidfVectorizer(max_df=0.75, max_features=700...</td>\n",
       "      <td>{'log_reg__C': 1, 'log_reg__penalty': 'l2', 't...</td>\n",
       "      <td>0.787606</td>\n",
       "      <td>0.746956</td>\n",
       "      <td>0.746956</td>\n",
       "      <td>0.531437</td>\n",
       "      <td>0.813032</td>\n",
       "      <td>0.702557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec</td>\n",
       "      <td>nb</td>\n",
       "      <td>(CountVectorizer(max_df=0.7, ngram_range=(1, 2...</td>\n",
       "      <td>{'cvec__max_df': 0.7, 'cvec__max_features': No...</td>\n",
       "      <td>0.959545</td>\n",
       "      <td>0.756992</td>\n",
       "      <td>0.756992</td>\n",
       "      <td>0.564097</td>\n",
       "      <td>0.818550</td>\n",
       "      <td>0.717254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec</td>\n",
       "      <td>nb</td>\n",
       "      <td>(TfidfVectorizer(max_df=0.75, min_df=3,\\n     ...</td>\n",
       "      <td>{'nb__alpha': 0.75, 'nb__fit_prior': True, 'tv...</td>\n",
       "      <td>0.769384</td>\n",
       "      <td>0.729434</td>\n",
       "      <td>0.729434</td>\n",
       "      <td>0.401466</td>\n",
       "      <td>0.811075</td>\n",
       "      <td>0.661870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cvec</td>\n",
       "      <td>rf</td>\n",
       "      <td>(CountVectorizer(max_df=0.9, min_df=3, ngram_r...</td>\n",
       "      <td>{'cvec__max_df': 0.9, 'cvec__max_features': No...</td>\n",
       "      <td>0.988688</td>\n",
       "      <td>0.739552</td>\n",
       "      <td>0.739552</td>\n",
       "      <td>0.510553</td>\n",
       "      <td>0.808701</td>\n",
       "      <td>0.692377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec</td>\n",
       "      <td>rf</td>\n",
       "      <td>(TfidfVectorizer(max_df=0.75, max_features=600...</td>\n",
       "      <td>{'rf__n_estimators': 100, 'tvec__max_df': 0.75...</td>\n",
       "      <td>0.985809</td>\n",
       "      <td>0.732149</td>\n",
       "      <td>0.732149</td>\n",
       "      <td>0.492335</td>\n",
       "      <td>0.804139</td>\n",
       "      <td>0.682745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vectorizer    Model                                     Best_Estimator  \\\n",
       "0       cvec  log_reg  (CountVectorizer(max_df=0.5, ngram_range=(1, 3...   \n",
       "1       tvec  log_reg  (TfidfVectorizer(max_df=0.75, max_features=700...   \n",
       "2       cvec       nb  (CountVectorizer(max_df=0.7, ngram_range=(1, 2...   \n",
       "3       tvec       nb  (TfidfVectorizer(max_df=0.75, min_df=3,\\n     ...   \n",
       "4       cvec       rf  (CountVectorizer(max_df=0.9, min_df=3, ngram_r...   \n",
       "5       tvec       rf  (TfidfVectorizer(max_df=0.75, max_features=600...   \n",
       "\n",
       "                                         Best_Params  Train Score  Test Score  \\\n",
       "0  {'cvec__max_df': 0.5, 'cvec__max_features': No...     0.975711    0.753620   \n",
       "1  {'log_reg__C': 1, 'log_reg__penalty': 'l2', 't...     0.787606    0.746956   \n",
       "2  {'cvec__max_df': 0.7, 'cvec__max_features': No...     0.959545    0.756992   \n",
       "3  {'nb__alpha': 0.75, 'nb__fit_prior': True, 'tv...     0.769384    0.729434   \n",
       "4  {'cvec__max_df': 0.9, 'cvec__max_features': No...     0.988688    0.739552   \n",
       "5  {'rf__n_estimators': 100, 'tvec__max_df': 0.75...     0.985809    0.732149   \n",
       "\n",
       "   Accuracy  Specificity   f_score   ROC_AUC  \n",
       "0  0.753620     0.538991  0.818099  0.709404  \n",
       "1  0.746956     0.531437  0.813032  0.702557  \n",
       "2  0.756992     0.564097  0.818550  0.717254  \n",
       "3  0.729434     0.401466  0.811075  0.661870  \n",
       "4  0.739552     0.510553  0.808701  0.692377  \n",
       "5  0.732149     0.492335  0.804139  0.682745  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_tuning_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Best Model and final tuning using GridSearchCV\n",
    "\n",
    "After tuning the hyperparameters using HalvingGridSearch, the **Naive Bayes model with CountVectorizer** transformer achieved the best F1 score of **0.819**. It also had the best ROC_AUC score of 0.717 and Accuracy of 76%, meaning the model is able to predict 76% of the test data correctly.\n",
    "I will now try to tune the model one last time using GridSearchCV to obtain the best parameters and see if the Accuracy, ROC-AUC and F1 scores can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run best model\n",
    "\n",
    "# Setting up pipeline with NB and CountVec\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "nb_cvec_params = {\n",
    "    'cvec__max_features': [None],\n",
    "    'cvec__min_df':[0.5, 0.75, 0.9, 1],\n",
    "    'cvec__max_df': [0.6, 0.7, 0.8, 0.9],\n",
    "    'cvec__ngram_range':[(1,1), (1,2)],\n",
    "    'nb__alpha': np.linspace(0, 1, 5),\n",
    "    'nb__fit_prior': [True, False]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GridSearchCV\n",
    "gs = GridSearchCV(pipe, nb_cvec_params, cv=5, verbose=True, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearch to training data\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Finding the Best Hyperparameter Values\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix\n",
    "It helps to categorize predictions into true positives, true negatives, false positives, and false negatives, and helps to compare the number of Type I and II errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1536752403.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[28], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    pipe = Pipeline([\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix on test data\n",
    "\n",
    "\n",
    "   pipe = Pipeline([\n",
    "        (vec, vectorizer),\n",
    "        (mod, models[mod])\n",
    "    ])\n",
    "\n",
    "    # Perform HalvingGridSearch\n",
    "\n",
    "    gs_best = HalvingGridSearchCV(pipe, param_grid=param_grid, cv=cv, n_jobs=-1, verbose=1, scoring = 'f1')\n",
    "    gs_best.fit(X_train, y_train)\n",
    "  \n",
    "    # Get the best model and best parameters\n",
    "    best_estimator = gs_best.best_estimator_\n",
    "    best_params = gs_best.best_params_\n",
    "\n",
    "    # Predict on test set\n",
    "    preds = best_estimator.predict(X_test)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "    acc = (tp + tn)/ (tp+tn+fp+fn)\n",
    "    spec = tn / (tn + fp)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(nb_cvec, X_test, y_test, cmap='Blues', ax=ax, values_format = 'd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC ROC Curve\n",
    "ROC is a probability curve and AUC represents the degree or measure of separability - hence telling us how capable the model is in distinguishing between the two classes - in our case, whether it is from AMD or Nvidia subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
